{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft model download\n",
    "# execute the following commands in shell or you will meet network problems\n",
    "!export HF_ENDPOINT=https://hf-mirror.com\n",
    "!huggingface-cli download --resume-download facebook/opt-350m --local-dir ./model/sft_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset download\n",
    "!huggingface-cli download --repo-type dataset --resume-download Dahoas/rm-static --local-dir ./data/rm_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm model download\n",
    "!huggingface-cli download --resume-download facebook/opt-125m --local-dir ./model/rm_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: SFT training\n",
    "adapting from https://github.com/deepspeedai/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-12 00:34:15,404] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/instructGPT/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/root/miniconda3/envs/instructGPT/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/root/miniconda3/envs/instructGPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the device\n",
    "import torch\n",
    "from deepspeed import get_accelerator\n",
    "device = torch.device(get_accelerator().device_name())\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-12 00:34:43,237] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-03-12 00:34:43,237] [INFO] [comm.py:673:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2025-03-12 00:34:43,309] [INFO] [comm.py:728:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.17.0.7, master_port=29500\n",
      "[2025-03-12 00:34:43,310] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W312 00:34:43.657286789 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n"
     ]
    }
   ],
   "source": [
    "# initialize deepspeed config\n",
    "from src.ds_utils import get_train_ds_config\n",
    "import deepspeed\n",
    "deepspeed.init_distributed()\n",
    "global_rank = torch.distributed.get_rank()\n",
    "ds_config = get_train_ds_config(offload=False,\n",
    "                                    dtype='fp16',\n",
    "                                    stage=0,\n",
    "                                    enable_tensorboard=True,\n",
    "                                    tb_path=\"step1_tensorboard\",\n",
    "                                    tb_name=\"step1_model\")\n",
    "per_device_train_batch_size = 24\n",
    "per_device_eval_batch_size = 24\n",
    "gradient_accumulation_steps = 1\n",
    "ds_config['train_micro_batch_size_per_gpu'] = per_device_train_batch_size\n",
    "ds_config['train_batch_size'] = per_device_train_batch_size * torch.distributed.get_world_size() * gradient_accumulation_steps\n",
    "torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='./model/sft_model', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "model_name_or_path = \"./model/sft_model\"\n",
    "model_json = os.path.join(model_name_or_path, \"config.json\")\n",
    "if os.path.exists(model_json):\n",
    "    model_json_file = json.load(open(model_json))\n",
    "    model_name = model_json_file.get(\"_name_or_path\",\n",
    "                                        model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, fast_tokenizer=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'right'\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize sft model\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "import math\n",
    "model_name_or_path = \"./model/sft_model\"\n",
    "model_config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "            config=model_config)\n",
    "model.config.end_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.resize_token_embeddings(int(8 *math.ceil(len(tokenizer) / 8.0)))\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): LinearLayer_LoRA(\n",
       "              (lora_dropout): Identity()\n",
       "            )\n",
       "            (v_proj): LinearLayer_LoRA(\n",
       "              (lora_dropout): Identity()\n",
       "            )\n",
       "            (q_proj): LinearLayer_LoRA(\n",
       "              (lora_dropout): Identity()\n",
       "            )\n",
       "            (out_proj): LinearLayer_LoRA(\n",
       "              (lora_dropout): Identity()\n",
       "            )\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): LinearLayer_LoRA(\n",
       "            (lora_dropout): Identity()\n",
       "          )\n",
       "          (fc2): LinearLayer_LoRA(\n",
       "            (lora_dropout): Identity()\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert model to lora form for efficient sft\n",
    "from src.lora import convert_linear_layer_to_lora\n",
    "lora_module_name = \"decoder.layers.\"\n",
    "lora_dim = 128\n",
    "model = convert_linear_layer_to_lora(model, lora_module_name, lora_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.data_utils.PromptDataset at 0x7f856108a4d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Prepare the data\n",
    "from src.data_utils import create_prompt_dataset\n",
    "train_phase = 1\n",
    "local_rank = -1\n",
    "data_path = [\"./data/rm_static\"]\n",
    "data_split = \"2,4,4\"\n",
    "data_output_path = \"./data/rm_static_processed4sft/\"\n",
    "max_seq_len = 512\n",
    "train_dataset, eval_dataset = create_prompt_dataset(\n",
    "    local_rank,\n",
    "    data_path,\n",
    "    data_split,\n",
    "    data_output_path,\n",
    "    train_phase,\n",
    "    1234,\n",
    "    tokenizer,\n",
    "    max_seq_len,\n",
    "    end_of_conversation_token=tokenizer.eos_token)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 50118, 50118,  ...,     2,     2,     2],\n",
       "         [    2, 50118, 50118,  ...,     2,     2,     2],\n",
       "         [    2, 50118, 50118,  ...,     2,     2,     2],\n",
       "         ...,\n",
       "         [    2, 50118, 50118,  ...,     2,     2,     2],\n",
       "         [    2, 50118, 50118,  ...,     2,     2,     2],\n",
       "         [    2, 50118, 50118,  ...,     2,     2,     2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[    2, 50118, 50118,  ...,  -100,  -100,  -100],\n",
       "         [    2, 50118, 50118,  ...,  -100,  -100,  -100],\n",
       "         [    2, 50118, 50118,  ...,  -100,  -100,  -100],\n",
       "         ...,\n",
       "         [    2, 50118, 50118,  ...,  -100,  -100,  -100],\n",
       "         [    2, 50118, 50118,  ...,  -100,  -100,  -100],\n",
       "         [    2, 50118, 50118,  ...,  -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataloader\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import default_data_collator\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                                  collate_fn=default_data_collator,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=per_device_train_batch_size)\n",
    "eval_dataloader = DataLoader(eval_dataset,\n",
    "                                collate_fn=default_data_collator,\n",
    "                                sampler=eval_sampler,\n",
    "                                batch_size=per_device_eval_batch_size)\n",
    "train_sample = next(iter(train_dataloader))\n",
    "train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import to_device, get_all_reduce_mean\n",
    "def evaluation(model, eval_dataloader):\n",
    "        model.eval()\n",
    "        losses = 0\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            batch = to_device(batch, device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            losses += loss.float()\n",
    "        losses = losses / (step + 1)\n",
    "        try:\n",
    "            losses = get_all_reduce_mean(losses)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            perplexity = torch.exp(losses).item()\n",
    "        except OverflowError:\n",
    "            perplexity = float(\"inf\")\n",
    "        return perplexity, losses.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...\n",
      "/root/miniconda3/envs/instructGPT/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load fused_adam op: 0.03557753562927246 seconds\n",
      "[2025-03-12 00:35:20,026] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown\n",
      "[2025-03-12 00:35:20,027] [INFO] [comm.py:683:init_distributed] Distributed backend already initialized\n",
      "[2025-03-12 00:35:20,028] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module fused_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-12 00:35:20,417] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-03-12 00:35:20,420] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-03-12 00:35:20,420] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-03-12 00:35:20,479] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-03-12 00:35:20,479] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
      "[2025-03-12 00:35:20,504] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = FP16_Optimizer\n",
      "[2025-03-12 00:35:20,505] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2025-03-12 00:35:20,505] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f855deaf580>\n",
      "[2025-03-12 00:35:20,506] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.0005, 0.001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:35:20,508] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:\n",
      "[2025-03-12 00:35:20,509] [INFO] [config.py:1005:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-03-12 00:35:20,509] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-03-12 00:35:20,510] [INFO] [config.py:1005:print]   amp_enabled .................. False\n",
      "[2025-03-12 00:35:20,510] [INFO] [config.py:1005:print]   amp_params ................... False\n",
      "[2025-03-12 00:35:20,511] [INFO] [config.py:1005:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-03-12 00:35:20,512] [INFO] [config.py:1005:print]   bfloat16_enabled ............. False\n",
      "[2025-03-12 00:35:20,513] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-03-12 00:35:20,513] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-03-12 00:35:20,514] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-03-12 00:35:20,514] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-03-12 00:35:20,515] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f855de955a0>\n",
      "[2025-03-12 00:35:20,515] [INFO] [config.py:1005:print]   communication_data_type ...... None\n",
      "[2025-03-12 00:35:20,516] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-03-12 00:35:20,516] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False\n",
      "[2025-03-12 00:35:20,517] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False\n",
      "[2025-03-12 00:35:20,518] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-03-12 00:35:20,518] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False\n",
      "[2025-03-12 00:35:20,519] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False\n",
      "[2025-03-12 00:35:20,519] [INFO] [config.py:1005:print]   disable_allgather ............ False\n",
      "[2025-03-12 00:35:20,520] [INFO] [config.py:1005:print]   dump_state ................... False\n",
      "[2025-03-12 00:35:20,521] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2025-03-12 00:35:20,521] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False\n",
      "[2025-03-12 00:35:20,522] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-03-12 00:35:20,522] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-03-12 00:35:20,523] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-03-12 00:35:20,524] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-03-12 00:35:20,524] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-03-12 00:35:20,525] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-03-12 00:35:20,525] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False\n",
      "[2025-03-12 00:35:20,526] [INFO] [config.py:1005:print]   elasticity_enabled ........... False\n",
      "[2025-03-12 00:35:20,526] [INFO] [config.py:1005:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-03-12 00:35:20,527] [INFO] [config.py:1005:print]   fp16_auto_cast ............... False\n",
      "[2025-03-12 00:35:20,528] [INFO] [config.py:1005:print]   fp16_enabled ................. True\n",
      "[2025-03-12 00:35:20,528] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-03-12 00:35:20,529] [INFO] [config.py:1005:print]   global_rank .................. 0\n",
      "[2025-03-12 00:35:20,529] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None\n",
      "[2025-03-12 00:35:20,530] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 1\n",
      "[2025-03-12 00:35:20,531] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0\n",
      "[2025-03-12 00:35:20,531] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-03-12 00:35:20,532] [INFO] [config.py:1005:print]   graph_harvesting ............. False\n",
      "[2025-03-12 00:35:20,532] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-03-12 00:35:20,533] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-03-12 00:35:20,534] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False\n",
      "[2025-03-12 00:35:20,534] [INFO] [config.py:1005:print]   loss_scale ................... 0\n",
      "[2025-03-12 00:35:20,535] [INFO] [config.py:1005:print]   memory_breakdown ............. False\n",
      "[2025-03-12 00:35:20,535] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False\n",
      "[2025-03-12 00:35:20,536] [INFO] [config.py:1005:print]   mics_shard_size .............. -1\n",
      "[2025-03-12 00:35:20,537] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=True, output_path='step1_tensorboard/ds_tensorboard_logs/', job_name='step1_model_tensorboard') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-03-12 00:35:20,537] [INFO] [config.py:1005:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-03-12 00:35:20,538] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-03-12 00:35:20,539] [INFO] [config.py:1005:print]   optimizer_name ............... None\n",
      "[2025-03-12 00:35:20,539] [INFO] [config.py:1005:print]   optimizer_params ............. None\n",
      "[2025-03-12 00:35:20,540] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-03-12 00:35:20,540] [INFO] [config.py:1005:print]   pld_enabled .................. False\n",
      "[2025-03-12 00:35:20,541] [INFO] [config.py:1005:print]   pld_params ................... False\n",
      "[2025-03-12 00:35:20,542] [INFO] [config.py:1005:print]   prescale_gradients ........... False\n",
      "[2025-03-12 00:35:20,542] [INFO] [config.py:1005:print]   scheduler_name ............... None\n",
      "[2025-03-12 00:35:20,543] [INFO] [config.py:1005:print]   scheduler_params ............. None\n",
      "[2025-03-12 00:35:20,544] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-03-12 00:35:20,544] [INFO] [config.py:1005:print]   sparse_attention ............. None\n",
      "[2025-03-12 00:35:20,545] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False\n",
      "[2025-03-12 00:35:20,545] [INFO] [config.py:1005:print]   steps_per_print .............. 10\n",
      "[2025-03-12 00:35:20,546] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-03-12 00:35:20,547] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-03-12 00:35:20,547] [INFO] [config.py:1005:print]   train_batch_size ............. 24\n",
      "[2025-03-12 00:35:20,548] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  24\n",
      "[2025-03-12 00:35:20,548] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False\n",
      "[2025-03-12 00:35:20,549] [INFO] [config.py:1005:print]   use_node_local_storage ....... False\n",
      "[2025-03-12 00:35:20,550] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False\n",
      "[2025-03-12 00:35:20,550] [INFO] [config.py:1005:print]   weight_quantization_config ... None\n",
      "[2025-03-12 00:35:20,551] [INFO] [config.py:1005:print]   world_size ................... 1\n",
      "[2025-03-12 00:35:20,552] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False\n",
      "[2025-03-12 00:35:20,552] [INFO] [config.py:1005:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-03-12 00:35:20,553] [INFO] [config.py:1005:print]   zero_enabled ................. False\n",
      "[2025-03-12 00:35:20,553] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-03-12 00:35:20,554] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 0\n",
      "[2025-03-12 00:35:20,555] [INFO] [config.py:991:print_user_config]   json = {\n",
      "    \"train_batch_size\": 24, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0, \n",
      "        \"overlap_comm\": true, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"stage3_max_live_parameters\": 3.000000e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 3.000000e+07, \n",
      "        \"memory_efficient_linear\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale_window\": 100\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"hybrid_engine\": {\n",
      "        \"enabled\": false, \n",
      "        \"max_out_tokens\": 512, \n",
      "        \"inference_tp_size\": 1, \n",
      "        \"release_inference_cache\": false, \n",
      "        \"pin_parameters\": true, \n",
      "        \"tp_gather_partition_size\": 8\n",
      "    }, \n",
      "    \"tensorboard\": {\n",
      "        \"enabled\": true, \n",
      "        \"output_path\": \"step1_tensorboard/ds_tensorboard_logs/\", \n",
      "        \"job_name\": \"step1_model_tensorboard\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "from src.utils import get_optimizer_grouped_parameters\n",
    "from deepspeed.ops.adam import FusedAdam\n",
    "from transformers import get_scheduler\n",
    "weight_decay = 0.\n",
    "lora_learning_rate = 5e-04\n",
    "optimizer_grouped_parameters = get_optimizer_grouped_parameters(model, weight_decay, lora_learning_rate)\n",
    "\n",
    "AdamOptimizer = FusedAdam\n",
    "learning_rate = 1e-03\n",
    "optimizer = AdamOptimizer(optimizer_grouped_parameters,\n",
    "                            lr=learning_rate,\n",
    "                            betas=(0.9, 0.95))\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / gradient_accumulation_steps)\n",
    "lr_scheduler_type = \"cosine\"\n",
    "num_warmup_steps = 0\n",
    "num_train_epochs = 1\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_train_epochs * num_update_steps_per_epoch,\n",
    ")\n",
    "\n",
    "model, optimizer, _, lr_scheduler = deepspeed.initialize(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    config=ds_config,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    dist_init_required=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sft opt-350 with batch_size=4 and max_seq_len=512 requires a gpu with more than 5GB RAM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "***** Evaluating perplexity, Epoch 0/1 *****\n",
      "ppl: 10.523646354675293, loss: 2.3536248207092285\n",
      "Beginning of Epoch 1/1, Total Micro Batches 636\n",
      "Epoch: 0, Step: 0, Rank: 0, loss = 2.3856611251831055\n",
      "Model Parameters: 0.388 B, Latency: 0.51s, TFLOPs: 39.79, Samples/sec: 47.22, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 1, Rank: 0, loss = 2.318455696105957\n",
      "Model Parameters: 0.388 B, Latency: 0.45s, TFLOPs: 44.63, Samples/sec: 52.96, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 2, Rank: 0, loss = 2.548229694366455\n",
      "Model Parameters: 0.388 B, Latency: 0.45s, TFLOPs: 44.47, Samples/sec: 52.77, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 3, Rank: 0, loss = 2.2974867820739746\n",
      "Model Parameters: 0.388 B, Latency: 0.45s, TFLOPs: 44.46, Samples/sec: 52.75, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 4, Rank: 0, loss = 2.3569300174713135\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 5, Rank: 0, loss = 2.487180233001709\n",
      "Model Parameters: 0.388 B, Latency: 0.45s, TFLOPs: 44.49, Samples/sec: 52.80, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 6, Rank: 0, loss = 2.5304689407348633\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.41, Samples/sec: 52.70, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 7, Rank: 0, loss = 2.5012190341949463\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 8, Rank: 0, loss = 2.50168514251709\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.40, Samples/sec: 52.69, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 9, Rank: 0, loss = 2.621206521987915\n",
      "[2025-03-12 00:35:54,220] [INFO] [logging.py:128:log_dist] [Rank 0] step=10, skipped=0, lr=[0.000999390130077636, 0.000499695065038818, 0.000999390130077636], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:35:54,222] [INFO] [timer.py:264:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=52.769372015364524, CurrSamplesPerSec=52.61409494107419, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.12, Samples/sec: 52.35, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 10, Rank: 0, loss = 2.543705701828003\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 43.98, Samples/sec: 52.19, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 11, Rank: 0, loss = 2.486433744430542\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.12, Samples/sec: 52.35, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 12, Rank: 0, loss = 2.5524816513061523\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.11, Samples/sec: 52.35, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 13, Rank: 0, loss = 2.6907546520233154\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.16, Samples/sec: 52.41, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 14, Rank: 0, loss = 2.7344624996185303\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.10, Samples/sec: 52.33, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 15, Rank: 0, loss = 2.4246790409088135\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 16, Rank: 0, loss = 2.643254280090332\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 17, Rank: 0, loss = 2.672912359237671\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.14, Samples/sec: 52.38, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 18, Rank: 0, loss = 2.6250455379486084\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.39, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 19, Rank: 0, loss = 2.6306869983673096\n",
      "[2025-03-12 00:35:58,845] [INFO] [logging.py:128:log_dist] [Rank 0] step=20, skipped=0, lr=[0.0009975620080758321, 0.0004987810040379161, 0.0009975620080758321], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:35:58,847] [INFO] [timer.py:264:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=52.553183470727866, CurrSamplesPerSec=51.66132807362157, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.47s, TFLOPs: 43.30, Samples/sec: 51.38, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 20, Rank: 0, loss = 2.4660604000091553\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.08, Samples/sec: 52.31, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 21, Rank: 0, loss = 2.68520450592041\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.10, Samples/sec: 52.33, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 22, Rank: 0, loss = 2.689409017562866\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.08, Samples/sec: 52.30, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 23, Rank: 0, loss = 2.7147412300109863\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 24, Rank: 0, loss = 2.7734787464141846\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 25, Rank: 0, loss = 2.7128610610961914\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.03, Samples/sec: 52.25, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 26, Rank: 0, loss = 2.5162718296051025\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 27, Rank: 0, loss = 2.574434995651245\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.12, Samples/sec: 52.35, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 28, Rank: 0, loss = 2.619976043701172\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 43.97, Samples/sec: 52.18, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 29, Rank: 0, loss = 2.578803062438965\n",
      "[2025-03-12 00:36:03,460] [INFO] [logging.py:128:log_dist] [Rank 0] step=30, skipped=0, lr=[0.000994520093661082, 0.000497260046830541, 0.000994520093661082], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:03,461] [INFO] [timer.py:264:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=52.52434917739028, CurrSamplesPerSec=52.69926265339009, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 30, Rank: 0, loss = 2.5184624195098877\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 31, Rank: 0, loss = 2.570220470428467\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 32, Rank: 0, loss = 2.615159034729004\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 33, Rank: 0, loss = 2.8025243282318115\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 34, Rank: 0, loss = 2.8111836910247803\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 35, Rank: 0, loss = 2.7682723999023438\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 36, Rank: 0, loss = 2.571584939956665\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 37, Rank: 0, loss = 2.6133639812469482\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 38, Rank: 0, loss = 2.6680498123168945\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 39, Rank: 0, loss = 2.6474976539611816\n",
      "[2025-03-12 00:36:08,052] [INFO] [logging.py:128:log_dist] [Rank 0] step=40, skipped=0, lr=[0.0009902718075218175, 0.0004951359037609087, 0.0009902718075218175], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:08,054] [INFO] [timer.py:264:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=52.570045249454566, CurrSamplesPerSec=52.80140205044682, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 40, Rank: 0, loss = 2.6749885082244873\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.40, Samples/sec: 52.69, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 41, Rank: 0, loss = 2.5638327598571777\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 42, Rank: 0, loss = 2.716564655303955\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 43, Rank: 0, loss = 2.5715274810791016\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.12, Samples/sec: 52.36, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 44, Rank: 0, loss = 2.6480915546417236\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.14, Samples/sec: 52.37, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 45, Rank: 0, loss = 2.7006044387817383\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.13, Samples/sec: 52.37, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 46, Rank: 0, loss = 2.6229820251464844\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 47, Rank: 0, loss = 2.673234462738037\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 48, Rank: 0, loss = 2.769704818725586\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 49, Rank: 0, loss = 2.6289780139923096\n",
      "[2025-03-12 00:36:12,656] [INFO] [logging.py:128:log_dist] [Rank 0] step=50, skipped=0, lr=[0.0009848275132657903, 0.0004924137566328951, 0.0009848275132657903], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:12,658] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=52.571498304360084, CurrSamplesPerSec=52.57013166931779, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.12, Samples/sec: 52.35, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 50, Rank: 0, loss = 2.7495343685150146\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 43.99, Samples/sec: 52.20, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 51, Rank: 0, loss = 2.6530189514160156\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 52, Rank: 0, loss = 2.5778846740722656\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 53, Rank: 0, loss = 2.5221192836761475\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 54, Rank: 0, loss = 2.634265184402466\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 55, Rank: 0, loss = 2.5373175144195557\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 56, Rank: 0, loss = 2.75142240524292\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 57, Rank: 0, loss = 2.701854705810547\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 43.81, Samples/sec: 51.99, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 58, Rank: 0, loss = 2.6475331783294678\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 59, Rank: 0, loss = 2.5271358489990234\n",
      "[2025-03-12 00:36:17,265] [INFO] [logging.py:128:log_dist] [Rank 0] step=60, skipped=0, lr=[0.0009782004921382612, 0.0004891002460691306, 0.0009782004921382612], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:17,266] [INFO] [timer.py:264:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=52.56495153214738, CurrSamplesPerSec=52.464098657021175, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.00, Samples/sec: 52.22, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 60, Rank: 0, loss = 2.7971866130828857\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 61, Rank: 0, loss = 2.517791271209717\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.14, Samples/sec: 52.38, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 62, Rank: 0, loss = 2.5783143043518066\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 63, Rank: 0, loss = 2.7604775428771973\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 64, Rank: 0, loss = 2.6898744106292725\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 65, Rank: 0, loss = 2.759640693664551\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 66, Rank: 0, loss = 2.7093770503997803\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 67, Rank: 0, loss = 2.852210760116577\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 68, Rank: 0, loss = 2.727358818054199\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 69, Rank: 0, loss = 2.668337345123291\n",
      "[2025-03-12 00:36:21,870] [INFO] [logging.py:128:log_dist] [Rank 0] step=70, skipped=0, lr=[0.0009704069106226727, 0.00048520345531133635, 0.0009704069106226727], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:21,871] [INFO] [timer.py:264:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=52.566225894240425, CurrSamplesPerSec=52.546778621138785, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.10, Samples/sec: 52.33, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 70, Rank: 0, loss = 2.564606189727783\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 71, Rank: 0, loss = 2.5514895915985107\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.13, Samples/sec: 52.36, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 72, Rank: 0, loss = 2.4252402782440186\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 73, Rank: 0, loss = 2.4004135131835938\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 74, Rank: 0, loss = 2.607790470123291\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 75, Rank: 0, loss = 2.666435956954956\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 76, Rank: 0, loss = 2.5806515216827393\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 77, Rank: 0, loss = 2.615633964538574\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 78, Rank: 0, loss = 2.602402925491333\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 79, Rank: 0, loss = 2.5215232372283936\n",
      "[2025-03-12 00:36:26,472] [INFO] [logging.py:128:log_dist] [Rank 0] step=80, skipped=0, lr=[0.0009614657810028401, 0.00048073289050142004, 0.0009614657810028401], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:26,474] [INFO] [timer.py:264:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=52.56862636820256, CurrSamplesPerSec=52.40109307749186, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.00, Samples/sec: 52.21, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 80, Rank: 0, loss = 2.4888274669647217\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 81, Rank: 0, loss = 2.5803630352020264\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 82, Rank: 0, loss = 2.6237165927886963\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.10, Samples/sec: 52.33, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 83, Rank: 0, loss = 2.44508957862854\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 84, Rank: 0, loss = 2.5342373847961426\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 85, Rank: 0, loss = 2.5869874954223633\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 86, Rank: 0, loss = 2.396996259689331\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 87, Rank: 0, loss = 2.5361154079437256\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 88, Rank: 0, loss = 2.5067169666290283\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 89, Rank: 0, loss = 2.3738269805908203\n",
      "[2025-03-12 00:36:31,073] [INFO] [logging.py:128:log_dist] [Rank 0] step=90, skipped=0, lr=[0.0009513989149828718, 0.0004756994574914359, 0.0009513989149828718], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:31,075] [INFO] [timer.py:264:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=52.5714900885071, CurrSamplesPerSec=52.323112917456235, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 43.91, Samples/sec: 52.11, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 90, Rank: 0, loss = 2.6262948513031006\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 91, Rank: 0, loss = 2.4980275630950928\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 92, Rank: 0, loss = 2.4601516723632812\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.13, Samples/sec: 52.37, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 93, Rank: 0, loss = 2.646260976791382\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 94, Rank: 0, loss = 2.5694925785064697\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 95, Rank: 0, loss = 2.63226318359375\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 96, Rank: 0, loss = 2.592567205429077\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 97, Rank: 0, loss = 2.555403470993042\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 98, Rank: 0, loss = 2.4830455780029297\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 99, Rank: 0, loss = 2.486415147781372\n",
      "[2025-03-12 00:36:35,669] [INFO] [logging.py:128:log_dist] [Rank 0] step=100, skipped=0, lr=[0.0009402308704779598, 0.0004701154352389799, 0.0009402308704779598], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:35,671] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=52.579711820840885, CurrSamplesPerSec=52.724380723725055, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 100, Rank: 0, loss = 2.4910168647766113\n",
      "[2025-03-12 00:36:36,117] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 100 iterations\n",
      "[2025-03-12 00:36:36,117] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 65536 to 131072\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 101, Rank: 0, loss = 2.551877737045288\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 102, Rank: 0, loss = 2.575289726257324\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 103, Rank: 0, loss = 2.5301051139831543\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 104, Rank: 0, loss = 2.646836519241333\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 105, Rank: 0, loss = 2.5396804809570312\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 106, Rank: 0, loss = 2.5143611431121826\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 107, Rank: 0, loss = 2.5223824977874756\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 108, Rank: 0, loss = 2.7044544219970703\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 109, Rank: 0, loss = 2.3869857788085938\n",
      "[2025-03-12 00:36:40,267] [INFO] [logging.py:128:log_dist] [Rank 0] step=110, skipped=0, lr=[0.0009279888917058453, 0.00046399444585292263, 0.0009279888917058453], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:40,268] [INFO] [timer.py:264:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=52.583991751582275, CurrSamplesPerSec=52.5051733057697, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.07, Samples/sec: 52.30, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 110, Rank: 0, loss = 2.699860095977783\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 111, Rank: 0, loss = 2.6394546031951904\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 112, Rank: 0, loss = 2.341376543045044\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 113, Rank: 0, loss = 2.5882160663604736\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 114, Rank: 0, loss = 2.575767993927002\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 115, Rank: 0, loss = 2.466885805130005\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 116, Rank: 0, loss = 2.4010655879974365\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 117, Rank: 0, loss = 2.5844039916992188\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 118, Rank: 0, loss = 2.580172300338745\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 119, Rank: 0, loss = 2.5510313510894775\n",
      "[2025-03-12 00:36:44,860] [INFO] [logging.py:128:log_dist] [Rank 0] step=120, skipped=0, lr=[0.000914702842725101, 0.0004573514213625505, 0.000914702842725101], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:44,862] [INFO] [timer.py:264:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=52.592178847826645, CurrSamplesPerSec=52.78741918701496, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 120, Rank: 0, loss = 2.7165656089782715\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 121, Rank: 0, loss = 2.509092330932617\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 122, Rank: 0, loss = 2.406306028366089\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 123, Rank: 0, loss = 2.633650541305542\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 124, Rank: 0, loss = 2.424329996109009\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 125, Rank: 0, loss = 2.658167839050293\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 126, Rank: 0, loss = 2.4339370727539062\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 127, Rank: 0, loss = 2.693580389022827\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 128, Rank: 0, loss = 2.6917309761047363\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 129, Rank: 0, loss = 2.2556450366973877\n",
      "[2025-03-12 00:36:49,449] [INFO] [logging.py:128:log_dist] [Rank 0] step=130, skipped=0, lr=[0.0009004051345823689, 0.00045020256729118443, 0.0009004051345823689], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:49,450] [INFO] [timer.py:264:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=52.601248664766224, CurrSamplesPerSec=52.705277769190104, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 130, Rank: 0, loss = 2.346055269241333\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 131, Rank: 0, loss = 2.5766632556915283\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 132, Rank: 0, loss = 2.550581693649292\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 133, Rank: 0, loss = 2.6781768798828125\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 134, Rank: 0, loss = 2.390165090560913\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 135, Rank: 0, loss = 2.417717933654785\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 136, Rank: 0, loss = 2.4503605365753174\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 137, Rank: 0, loss = 2.487548828125\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 138, Rank: 0, loss = 2.578448534011841\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 139, Rank: 0, loss = 2.325576066970825\n",
      "[2025-03-12 00:36:54,045] [INFO] [logging.py:128:log_dist] [Rank 0] step=140, skipped=0, lr=[0.0008851306462462688, 0.0004425653231231344, 0.0008851306462462688], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:54,046] [INFO] [timer.py:264:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=52.604025047761276, CurrSamplesPerSec=52.56447673604988, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.10, Samples/sec: 52.33, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 140, Rank: 0, loss = 2.3778016567230225\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.08, Samples/sec: 52.31, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 141, Rank: 0, loss = 2.3767130374908447\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.14, Samples/sec: 52.38, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 142, Rank: 0, loss = 2.57110857963562\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 143, Rank: 0, loss = 2.6356313228607178\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 144, Rank: 0, loss = 2.6700010299682617\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.09, Samples/sec: 52.32, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 145, Rank: 0, loss = 2.5768253803253174\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 43.97, Samples/sec: 52.18, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 146, Rank: 0, loss = 2.61893892288208\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 147, Rank: 0, loss = 2.675858497619629\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 148, Rank: 0, loss = 2.523620367050171\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 149, Rank: 0, loss = 2.415738821029663\n",
      "[2025-03-12 00:36:58,652] [INFO] [logging.py:128:log_dist] [Rank 0] step=150, skipped=0, lr=[0.0008689166395208636, 0.0004344583197604318, 0.0008689166395208636], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:36:58,654] [INFO] [timer.py:264:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=52.59950664915874, CurrSamplesPerSec=52.71745016163429, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 150, Rank: 0, loss = 2.4184679985046387\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 151, Rank: 0, loss = 2.469149589538574\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.03, Samples/sec: 52.25, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 152, Rank: 0, loss = 2.4766764640808105\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 153, Rank: 0, loss = 2.631643056869507\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 154, Rank: 0, loss = 2.6615278720855713\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 155, Rank: 0, loss = 2.44311785697937\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 156, Rank: 0, loss = 2.490203619003296\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 157, Rank: 0, loss = 2.447646379470825\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 158, Rank: 0, loss = 2.562884569168091\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 159, Rank: 0, loss = 2.508040189743042\n",
      "[2025-03-12 00:37:03,246] [INFO] [logging.py:128:log_dist] [Rank 0] step=160, skipped=0, lr=[0.0008518026681462447, 0.00042590133407312237, 0.0008518026681462447], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:03,248] [INFO] [timer.py:264:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=52.60390549317632, CurrSamplesPerSec=52.685692299095514, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 160, Rank: 0, loss = 2.484848737716675\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 161, Rank: 0, loss = 2.537912130355835\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 162, Rank: 0, loss = 2.5802133083343506\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 163, Rank: 0, loss = 2.5244414806365967\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 164, Rank: 0, loss = 2.352847099304199\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.43, Samples/sec: 52.72, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 165, Rank: 0, loss = 2.484941005706787\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.41, Samples/sec: 52.70, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 166, Rank: 0, loss = 2.547640562057495\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.45, Samples/sec: 52.74, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 167, Rank: 0, loss = 2.5559167861938477\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.43, Samples/sec: 52.73, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 168, Rank: 0, loss = 2.5013091564178467\n",
      "Model Parameters: 0.388 B, Latency: 0.45s, TFLOPs: 44.45, Samples/sec: 52.75, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 169, Rank: 0, loss = 2.4158246517181396\n",
      "[2025-03-12 00:37:07,830] [INFO] [logging.py:128:log_dist] [Rank 0] step=170, skipped=0, lr=[0.0008338304813079864, 0.0004169152406539932, 0.0008338304813079864], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:07,832] [INFO] [timer.py:264:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=52.613175939588466, CurrSamplesPerSec=52.77994623848738, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 170, Rank: 0, loss = 2.521503448486328\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.42, Samples/sec: 52.71, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 171, Rank: 0, loss = 2.656926155090332\n",
      "Model Parameters: 0.388 B, Latency: 0.45s, TFLOPs: 44.50, Samples/sec: 52.80, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 172, Rank: 0, loss = 2.4824841022491455\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.43, Samples/sec: 52.72, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 173, Rank: 0, loss = 2.3795125484466553\n",
      "Model Parameters: 0.388 B, Latency: 0.45s, TFLOPs: 44.45, Samples/sec: 52.75, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 174, Rank: 0, loss = 2.6612071990966797\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 175, Rank: 0, loss = 2.710996150970459\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 176, Rank: 0, loss = 2.6001760959625244\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 177, Rank: 0, loss = 2.392202854156494\n",
      "Model Parameters: 0.388 B, Latency: 0.45s, TFLOPs: 44.46, Samples/sec: 52.76, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 178, Rank: 0, loss = 2.5452640056610107\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 179, Rank: 0, loss = 2.528201103210449\n",
      "[2025-03-12 00:37:12,414] [INFO] [logging.py:128:log_dist] [Rank 0] step=180, skipped=0, lr=[0.0008150439217908557, 0.00040752196089542783, 0.0008150439217908557], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:12,416] [INFO] [timer.py:264:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=52.621405028046105, CurrSamplesPerSec=52.71038341712035, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 180, Rank: 0, loss = 2.6207070350646973\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.40, Samples/sec: 52.69, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 181, Rank: 0, loss = 2.707051992416382\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.40, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 182, Rank: 0, loss = 2.5161964893341064\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 183, Rank: 0, loss = 2.5250279903411865\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 184, Rank: 0, loss = 2.332770347595215\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 185, Rank: 0, loss = 2.569581985473633\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.44, Samples/sec: 52.73, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 186, Rank: 0, loss = 2.5094919204711914\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 187, Rank: 0, loss = 2.678272008895874\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 188, Rank: 0, loss = 2.519866943359375\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.41, Samples/sec: 52.70, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 189, Rank: 0, loss = 2.3323378562927246\n",
      "[2025-03-12 00:37:17,003] [INFO] [logging.py:128:log_dist] [Rank 0] step=190, skipped=0, lr=[0.0007954888190252292, 0.0003977444095126146, 0.0007954888190252292], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:17,005] [INFO] [timer.py:264:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=52.62666904410432, CurrSamplesPerSec=52.72609293605991, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 190, Rank: 0, loss = 2.6055691242218018\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 191, Rank: 0, loss = 2.544675350189209\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 192, Rank: 0, loss = 2.618896007537842\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 193, Rank: 0, loss = 2.4746253490448\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 194, Rank: 0, loss = 2.4009225368499756\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 195, Rank: 0, loss = 2.3899641036987305\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 196, Rank: 0, loss = 2.5287163257598877\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.41, Samples/sec: 52.70, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 197, Rank: 0, loss = 2.48241925239563\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.40, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 198, Rank: 0, loss = 2.3784923553466797\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.39, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 199, Rank: 0, loss = 2.3848795890808105\n",
      "[2025-03-12 00:37:21,592] [INFO] [logging.py:128:log_dist] [Rank 0] step=200, skipped=0, lr=[0.0007752128772871292, 0.0003876064386435646, 0.0007752128772871292], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:21,594] [INFO] [timer.py:264:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=52.63120002472893, CurrSamplesPerSec=52.78235395751306, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 200, Rank: 0, loss = 2.5555593967437744\n",
      "[2025-03-12 00:37:22,041] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 100 iterations\n",
      "[2025-03-12 00:37:22,042] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 131072 to 262144\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.16, Samples/sec: 52.40, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 201, Rank: 0, loss = 2.557771921157837\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 202, Rank: 0, loss = 2.4747302532196045\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.40, Samples/sec: 52.69, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 203, Rank: 0, loss = 2.5101728439331055\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 204, Rank: 0, loss = 2.60339617729187\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 205, Rank: 0, loss = 2.3509509563446045\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.42, Samples/sec: 52.71, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 206, Rank: 0, loss = 2.6536431312561035\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 207, Rank: 0, loss = 2.5438759326934814\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 208, Rank: 0, loss = 2.2677981853485107\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 209, Rank: 0, loss = 2.745863199234009\n",
      "[2025-03-12 00:37:26,182] [INFO] [logging.py:128:log_dist] [Rank 0] step=210, skipped=0, lr=[0.0007542655593246102, 0.0003771327796623051, 0.0007542655593246102], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:26,184] [INFO] [timer.py:264:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=52.63488740997467, CurrSamplesPerSec=52.80467040115307, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 210, Rank: 0, loss = 2.396821975708008\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 211, Rank: 0, loss = 2.580116033554077\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 212, Rank: 0, loss = 2.4599153995513916\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.41, Samples/sec: 52.70, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 213, Rank: 0, loss = 2.483896493911743\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 214, Rank: 0, loss = 2.3177003860473633\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.42, Samples/sec: 52.70, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 215, Rank: 0, loss = 2.3508834838867188\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 216, Rank: 0, loss = 2.628031015396118\n",
      "Model Parameters: 0.388 B, Latency: 0.48s, TFLOPs: 42.57, Samples/sec: 50.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 217, Rank: 0, loss = 2.41318678855896\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.16, Samples/sec: 52.40, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 218, Rank: 0, loss = 2.3295438289642334\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 219, Rank: 0, loss = 2.626126289367676\n",
      "[2025-03-12 00:37:30,794] [INFO] [logging.py:128:log_dist] [Rank 0] step=220, skipped=0, lr=[0.0007326979656943906, 0.0003663489828471953, 0.0007326979656943906], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:30,796] [INFO] [timer.py:264:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=52.62738804559421, CurrSamplesPerSec=52.6408660785347, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.38, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 220, Rank: 0, loss = 2.2724993228912354\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 221, Rank: 0, loss = 2.427189826965332\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.41, Samples/sec: 52.70, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 222, Rank: 0, loss = 2.5466084480285645\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 223, Rank: 0, loss = 2.5171561241149902\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.42, Samples/sec: 52.71, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 224, Rank: 0, loss = 2.3870909214019775\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.41, Samples/sec: 52.70, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 225, Rank: 0, loss = 2.5403928756713867\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 226, Rank: 0, loss = 2.45017671585083\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 227, Rank: 0, loss = 2.4917006492614746\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 228, Rank: 0, loss = 2.5774312019348145\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 229, Rank: 0, loss = 2.4511682987213135\n",
      "[2025-03-12 00:37:35,386] [INFO] [logging.py:128:log_dist] [Rank 0] step=230, skipped=0, lr=[0.0007105627101030816, 0.0003552813550515408, 0.0007105627101030816], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:35,388] [INFO] [timer.py:264:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=52.63054211578548, CurrSamplesPerSec=52.65738807189485, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.39, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 230, Rank: 0, loss = 2.4602861404418945\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 231, Rank: 0, loss = 2.3714306354522705\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 232, Rank: 0, loss = 2.425499439239502\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 233, Rank: 0, loss = 2.47114896774292\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 234, Rank: 0, loss = 2.517273426055908\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 235, Rank: 0, loss = 2.506852626800537\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 236, Rank: 0, loss = 2.409710168838501\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.40, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 237, Rank: 0, loss = 2.5526864528656006\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 238, Rank: 0, loss = 2.471264123916626\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 239, Rank: 0, loss = 2.4038026332855225\n",
      "[2025-03-12 00:37:39,975] [INFO] [logging.py:128:log_dist] [Rank 0] step=240, skipped=0, lr=[0.0006879137910571191, 0.00034395689552855954, 0.0006879137910571191], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:39,977] [INFO] [timer.py:264:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=52.63453912499151, CurrSamplesPerSec=52.717394945348886, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 240, Rank: 0, loss = 2.4836273193359375\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 241, Rank: 0, loss = 2.4293367862701416\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.42, Samples/sec: 52.70, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 242, Rank: 0, loss = 2.4607794284820557\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 243, Rank: 0, loss = 2.652695417404175\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.42, Samples/sec: 52.72, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 244, Rank: 0, loss = 2.4540634155273438\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 245, Rank: 0, loss = 2.3423709869384766\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 246, Rank: 0, loss = 2.4486725330352783\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 247, Rank: 0, loss = 2.4952735900878906\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 248, Rank: 0, loss = 2.446127414703369\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.41, Samples/sec: 52.70, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 249, Rank: 0, loss = 2.5164833068847656\n",
      "[2025-03-12 00:37:44,563] [INFO] [logging.py:128:log_dist] [Rank 0] step=250, skipped=0, lr=[0.000664806460134504, 0.000332403230067252, 0.000664806460134504], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:44,565] [INFO] [timer.py:264:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=52.63844152563828, CurrSamplesPerSec=52.75707003374134, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 250, Rank: 0, loss = 2.551820755004883\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 251, Rank: 0, loss = 2.460245132446289\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 252, Rank: 0, loss = 2.5634965896606445\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.40, Samples/sec: 52.69, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 253, Rank: 0, loss = 2.4400148391723633\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 254, Rank: 0, loss = 2.3295705318450928\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 255, Rank: 0, loss = 2.3174757957458496\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 256, Rank: 0, loss = 2.5320518016815186\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 257, Rank: 0, loss = 2.335148811340332\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 258, Rank: 0, loss = 2.3279078006744385\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 259, Rank: 0, loss = 2.618320941925049\n",
      "[2025-03-12 00:37:49,154] [INFO] [logging.py:128:log_dist] [Rank 0] step=260, skipped=0, lr=[0.0006412970871996995, 0.00032064854359984974, 0.0006412970871996995], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:49,156] [INFO] [timer.py:264:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=52.64145976534371, CurrSamplesPerSec=52.74656524548223, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 260, Rank: 0, loss = 2.540381908416748\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 261, Rank: 0, loss = 2.5181756019592285\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.40, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 262, Rank: 0, loss = 2.5054943561553955\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 263, Rank: 0, loss = 2.573941469192505\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 264, Rank: 0, loss = 2.397052526473999\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 265, Rank: 0, loss = 2.516179084777832\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 266, Rank: 0, loss = 2.4987399578094482\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.65, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 267, Rank: 0, loss = 2.3097879886627197\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 268, Rank: 0, loss = 2.397115468978882\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 269, Rank: 0, loss = 2.404482364654541\n",
      "[2025-03-12 00:37:53,745] [INFO] [logging.py:128:log_dist] [Rank 0] step=270, skipped=0, lr=[0.0006174430228904919, 0.00030872151144524594, 0.0006174430228904919], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:53,747] [INFO] [timer.py:264:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=52.643382246426945, CurrSamplesPerSec=52.749246333250774, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 270, Rank: 0, loss = 2.661393880844116\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 271, Rank: 0, loss = 2.5237979888916016\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 272, Rank: 0, loss = 2.694559335708618\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 273, Rank: 0, loss = 2.4210996627807617\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.40, Samples/sec: 52.69, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 274, Rank: 0, loss = 2.583966016769409\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.43, Samples/sec: 52.72, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 275, Rank: 0, loss = 2.4318745136260986\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.67, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 276, Rank: 0, loss = 2.452373743057251\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 277, Rank: 0, loss = 2.52437686920166\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 278, Rank: 0, loss = 2.5917725563049316\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 279, Rank: 0, loss = 2.417642593383789\n",
      "[2025-03-12 00:37:58,350] [INFO] [logging.py:128:log_dist] [Rank 0] step=280, skipped=0, lr=[0.0005933024587122745, 0.00029665122935613726, 0.0005933024587122745], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:37:58,352] [INFO] [timer.py:264:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=52.63876383571382, CurrSamplesPerSec=50.91778311354714, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.47s, TFLOPs: 42.74, Samples/sec: 50.72, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 280, Rank: 0, loss = 2.464454174041748\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 281, Rank: 0, loss = 2.512772798538208\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.17, Samples/sec: 52.42, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 282, Rank: 0, loss = 2.3775601387023926\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.05, Samples/sec: 52.27, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 283, Rank: 0, loss = 2.5958642959594727\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 284, Rank: 0, loss = 2.5073349475860596\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 285, Rank: 0, loss = 2.4626734256744385\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 286, Rank: 0, loss = 2.3611063957214355\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.18, Samples/sec: 52.43, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 287, Rank: 0, loss = 2.4758448600769043\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.14, Samples/sec: 52.38, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 288, Rank: 0, loss = 2.333991527557373\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 289, Rank: 0, loss = 2.4192233085632324\n",
      "[2025-03-12 00:38:02,955] [INFO] [logging.py:128:log_dist] [Rank 0] step=290, skipped=0, lr=[0.0005689342850810523, 0.00028446714254052615, 0.0005689342850810523], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:02,957] [INFO] [timer.py:264:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=52.63523385273762, CurrSamplesPerSec=52.60056840802995, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.12, Samples/sec: 52.35, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 290, Rank: 0, loss = 2.352283239364624\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 291, Rank: 0, loss = 2.3712451457977295\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 292, Rank: 0, loss = 2.42392635345459\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 293, Rank: 0, loss = 2.3297948837280273\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.13, Samples/sec: 52.36, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 294, Rank: 0, loss = 2.5188488960266113\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 295, Rank: 0, loss = 2.608908176422119\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 296, Rank: 0, loss = 2.4839744567871094\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.17, Samples/sec: 52.42, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 297, Rank: 0, loss = 2.413059949874878\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 298, Rank: 0, loss = 2.2750422954559326\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 299, Rank: 0, loss = 2.5121726989746094\n",
      "[2025-03-12 00:38:07,561] [INFO] [logging.py:128:log_dist] [Rank 0] step=300, skipped=0, lr=[0.0005443979476614675, 0.00027219897383073374, 0.0005443979476614675], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:07,563] [INFO] [timer.py:264:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=52.63206005592361, CurrSamplesPerSec=52.4902793954631, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.01, Samples/sec: 52.23, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 300, Rank: 0, loss = 2.647010564804077\n",
      "[2025-03-12 00:38:08,013] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 100 iterations\n",
      "[2025-03-12 00:38:08,013] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 262144 to 524288\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 43.89, Samples/sec: 52.08, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 301, Rank: 0, loss = 2.299539566040039\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.10, Samples/sec: 52.33, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 302, Rank: 0, loss = 2.4174561500549316\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 303, Rank: 0, loss = 2.3101186752319336\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 304, Rank: 0, loss = 2.4766173362731934\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 305, Rank: 0, loss = 2.436490535736084\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 306, Rank: 0, loss = 2.4501726627349854\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 307, Rank: 0, loss = 2.4376487731933594\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 308, Rank: 0, loss = 2.519516944885254\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 309, Rank: 0, loss = 2.3212900161743164\n",
      "[2025-03-12 00:38:12,169] [INFO] [logging.py:128:log_dist] [Rank 0] step=310, skipped=0, lr=[0.0005197533023503089, 0.00025987665117515446, 0.0005197533023503089], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:12,171] [INFO] [timer.py:264:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=52.62773400928129, CurrSamplesPerSec=52.55004295813594, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.08, Samples/sec: 52.30, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 310, Rank: 0, loss = 2.382230043411255\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.18, Samples/sec: 52.43, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 311, Rank: 0, loss = 2.4922351837158203\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.11, Samples/sec: 52.34, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 312, Rank: 0, loss = 2.5647966861724854\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 313, Rank: 0, loss = 2.3703722953796387\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 314, Rank: 0, loss = 2.45916748046875\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 315, Rank: 0, loss = 2.366640567779541\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 316, Rank: 0, loss = 2.4569809436798096\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.39, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 317, Rank: 0, loss = 2.5896155834198\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 318, Rank: 0, loss = 2.3662590980529785\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 319, Rank: 0, loss = 2.4273571968078613\n",
      "[2025-03-12 00:38:16,774] [INFO] [logging.py:128:log_dist] [Rank 0] step=320, skipped=0, lr=[0.0004950604692592673, 0.00024753023462963363, 0.0004950604692592673], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:16,776] [INFO] [timer.py:264:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=52.624934586110875, CurrSamplesPerSec=52.560798937345005, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.11, Samples/sec: 52.34, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 320, Rank: 0, loss = 2.3740715980529785\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 321, Rank: 0, loss = 2.3293206691741943\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 322, Rank: 0, loss = 2.432745933532715\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 323, Rank: 0, loss = 2.4371962547302246\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 324, Rank: 0, loss = 2.370973587036133\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 325, Rank: 0, loss = 2.307642936706543\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 326, Rank: 0, loss = 2.5002975463867188\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.10, Samples/sec: 52.32, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 327, Rank: 0, loss = 2.371267557144165\n",
      "[2025-03-12 00:38:20,428] [INFO] [fused_optimizer.py:392:_update_scale] \n",
      "Grad overflow on iteration 327\n",
      "[2025-03-12 00:38:20,429] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0\n",
      "[2025-03-12 00:38:20,429] [INFO] [logging.py:128:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288, reducing to 262144.0\n",
      "Model Parameters: 0.388 B, Latency: 0.43s, TFLOPs: 46.96, Samples/sec: 55.72, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 328, Rank: 0, loss = 2.2676308155059814\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.40, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 329, Rank: 0, loss = 2.5474042892456055\n",
      "[2025-03-12 00:38:21,352] [INFO] [logging.py:128:log_dist] [Rank 0] step=330, skipped=1, lr=[0.0004728455052947732, 0.0002364227526473866, 0.0004728455052947732], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:21,354] [INFO] [timer.py:264:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=52.63152069521204, CurrSamplesPerSec=52.566617781253505, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.09, Samples/sec: 52.31, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 330, Rank: 0, loss = 2.4350736141204834\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 331, Rank: 0, loss = 2.4457719326019287\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.09, Samples/sec: 52.32, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 332, Rank: 0, loss = 2.322711229324341\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 333, Rank: 0, loss = 2.5650205612182617\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 334, Rank: 0, loss = 2.581042528152466\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 335, Rank: 0, loss = 2.446856737136841\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 336, Rank: 0, loss = 2.453852653503418\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 337, Rank: 0, loss = 2.3392505645751953\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 338, Rank: 0, loss = 2.5631065368652344\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 339, Rank: 0, loss = 2.379338264465332\n",
      "[2025-03-12 00:38:25,951] [INFO] [logging.py:128:log_dist] [Rank 0] step=340, skipped=1, lr=[0.0004482270489293685, 0.00022411352446468426, 0.0004482270489293685], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:25,953] [INFO] [timer.py:264:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=52.631155503684525, CurrSamplesPerSec=52.717643419544075, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 340, Rank: 0, loss = 2.4020731449127197\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 341, Rank: 0, loss = 2.345355272293091\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 342, Rank: 0, loss = 2.3155019283294678\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 343, Rank: 0, loss = 2.305985689163208\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.38, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 344, Rank: 0, loss = 2.3329412937164307\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 345, Rank: 0, loss = 2.3876545429229736\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 346, Rank: 0, loss = 2.5041122436523438\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 347, Rank: 0, loss = 2.326004981994629\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 348, Rank: 0, loss = 2.5479342937469482\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 349, Rank: 0, loss = 2.5177860260009766\n",
      "[2025-03-12 00:38:30,546] [INFO] [logging.py:128:log_dist] [Rank 0] step=350, skipped=1, lr=[0.00042373489162656375, 0.00021186744581328188, 0.00042373489162656375], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:30,548] [INFO] [timer.py:264:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=52.631261235609415, CurrSamplesPerSec=52.62042070688768, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.12, Samples/sec: 52.36, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 350, Rank: 0, loss = 2.4489951133728027\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 351, Rank: 0, loss = 2.4030423164367676\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 352, Rank: 0, loss = 2.4010303020477295\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 353, Rank: 0, loss = 2.500136137008667\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 354, Rank: 0, loss = 2.471008062362671\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 355, Rank: 0, loss = 2.294152021408081\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 356, Rank: 0, loss = 2.294212818145752\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 357, Rank: 0, loss = 2.259187936782837\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 358, Rank: 0, loss = 2.4657201766967773\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 359, Rank: 0, loss = 2.4429056644439697\n",
      "[2025-03-12 00:38:35,140] [INFO] [logging.py:128:log_dist] [Rank 0] step=360, skipped=1, lr=[0.0003994287815066503, 0.00019971439075332516, 0.0003994287815066503], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:35,142] [INFO] [timer.py:264:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=52.6317650726056, CurrSamplesPerSec=52.704560297940745, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 360, Rank: 0, loss = 2.3629493713378906\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 361, Rank: 0, loss = 2.385780096054077\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 362, Rank: 0, loss = 2.39581298828125\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 363, Rank: 0, loss = 2.28702712059021\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 364, Rank: 0, loss = 2.367460012435913\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 365, Rank: 0, loss = 2.3783819675445557\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 366, Rank: 0, loss = 2.5226528644561768\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 367, Rank: 0, loss = 2.4019041061401367\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 368, Rank: 0, loss = 2.3169937133789062\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 369, Rank: 0, loss = 2.2998645305633545\n",
      "[2025-03-12 00:38:39,735] [INFO] [logging.py:128:log_dist] [Rank 0] step=370, skipped=1, lr=[0.0003753680128315952, 0.0001876840064157976, 0.0003753680128315952], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:39,737] [INFO] [timer.py:264:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=52.63222306540664, CurrSamplesPerSec=52.47528451187345, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.05, Samples/sec: 52.27, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 370, Rank: 0, loss = 2.3936851024627686\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 371, Rank: 0, loss = 2.408940076828003\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 372, Rank: 0, loss = 2.431398630142212\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 373, Rank: 0, loss = 2.352494478225708\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 374, Rank: 0, loss = 2.433255434036255\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 375, Rank: 0, loss = 2.5755162239074707\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 376, Rank: 0, loss = 2.378622531890869\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 377, Rank: 0, loss = 2.5841972827911377\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 378, Rank: 0, loss = 2.2519702911376953\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 379, Rank: 0, loss = 2.409295082092285\n",
      "[2025-03-12 00:38:44,329] [INFO] [logging.py:128:log_dist] [Rank 0] step=380, skipped=1, lr=[0.0003516112813578941, 0.00017580564067894706, 0.0003516112813578941], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:44,331] [INFO] [timer.py:264:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=52.6324387898731, CurrSamplesPerSec=52.66201608873026, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 380, Rank: 0, loss = 2.3753676414489746\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 381, Rank: 0, loss = 2.400808334350586\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 382, Rank: 0, loss = 2.3973875045776367\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 383, Rank: 0, loss = 2.395935535430908\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 384, Rank: 0, loss = 2.463819980621338\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 385, Rank: 0, loss = 2.4495811462402344\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 386, Rank: 0, loss = 2.630558490753174\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 387, Rank: 0, loss = 2.3843941688537598\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 388, Rank: 0, loss = 2.414943218231201\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 389, Rank: 0, loss = 2.253711223602295\n",
      "[2025-03-12 00:38:48,920] [INFO] [logging.py:128:log_dist] [Rank 0] step=390, skipped=1, lr=[0.00032821654114946496, 0.00016410827057473248, 0.00032821654114946496], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:48,922] [INFO] [timer.py:264:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=52.63336862788028, CurrSamplesPerSec=52.643894334445044, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 390, Rank: 0, loss = 2.4163525104522705\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 391, Rank: 0, loss = 2.332472562789917\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 392, Rank: 0, loss = 2.469447374343872\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 393, Rank: 0, loss = 2.4139208793640137\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 394, Rank: 0, loss = 2.5580978393554688\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 395, Rank: 0, loss = 2.5219361782073975\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 396, Rank: 0, loss = 2.4810597896575928\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 397, Rank: 0, loss = 2.345431327819824\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 398, Rank: 0, loss = 2.261150598526001\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 399, Rank: 0, loss = 2.2957513332366943\n",
      "[2025-03-12 00:38:53,514] [INFO] [logging.py:128:log_dist] [Rank 0] step=400, skipped=1, lr=[0.0003052408631998863, 0.00015262043159994316, 0.0003052408631998863], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:53,516] [INFO] [timer.py:264:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=52.63363642557797, CurrSamplesPerSec=52.62969207659881, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.18, Samples/sec: 52.43, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 400, Rank: 0, loss = 2.4291255474090576\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 401, Rank: 0, loss = 2.38448429107666\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 402, Rank: 0, loss = 2.6488006114959717\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 403, Rank: 0, loss = 2.4270431995391846\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.43, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 404, Rank: 0, loss = 2.2144076824188232\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 405, Rank: 0, loss = 2.5015745162963867\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 406, Rank: 0, loss = 2.12872576713562\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 407, Rank: 0, loss = 2.44219708442688\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 408, Rank: 0, loss = 2.359760046005249\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.39, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 409, Rank: 0, loss = 2.285053253173828\n",
      "[2025-03-12 00:38:58,112] [INFO] [logging.py:128:log_dist] [Rank 0] step=410, skipped=1, lr=[0.00028274029620886773, 0.00014137014810443386, 0.00028274029620886773], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:38:58,114] [INFO] [timer.py:264:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=52.63306704114834, CurrSamplesPerSec=52.6390217661094, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 410, Rank: 0, loss = 2.4307892322540283\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 411, Rank: 0, loss = 2.533637046813965\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 412, Rank: 0, loss = 2.531604528427124\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 413, Rank: 0, loss = 2.291050434112549\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 414, Rank: 0, loss = 2.3740861415863037\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 415, Rank: 0, loss = 2.28531813621521\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 416, Rank: 0, loss = 2.364482879638672\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 417, Rank: 0, loss = 2.4029386043548584\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 418, Rank: 0, loss = 2.401353120803833\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 419, Rank: 0, loss = 2.376174211502075\n",
      "[2025-03-12 00:39:02,708] [INFO] [logging.py:128:log_dist] [Rank 0] step=420, skipped=1, lr=[0.000260769729852585, 0.0001303848649262925, 0.000260769729852585], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:02,710] [INFO] [timer.py:264:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=52.63291169859748, CurrSamplesPerSec=52.667140913521585, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 420, Rank: 0, loss = 2.3653945922851562\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 421, Rank: 0, loss = 2.1770083904266357\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 422, Rank: 0, loss = 2.3434083461761475\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 423, Rank: 0, loss = 2.432530403137207\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 424, Rank: 0, loss = 2.376941442489624\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 425, Rank: 0, loss = 2.03251576423645\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 426, Rank: 0, loss = 2.3444957733154297\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 427, Rank: 0, loss = 2.450188636779785\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 428, Rank: 0, loss = 2.4261462688446045\n",
      "[2025-03-12 00:39:06,832] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 100 iterations\n",
      "[2025-03-12 00:39:06,833] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.12, Samples/sec: 52.36, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 429, Rank: 0, loss = 2.3237836360931396\n",
      "[2025-03-12 00:39:07,305] [INFO] [logging.py:128:log_dist] [Rank 0] step=430, skipped=1, lr=[0.00023938276088143003, 0.00011969138044071501, 0.00023938276088143003], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:07,306] [INFO] [timer.py:264:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=52.63241242259043, CurrSamplesPerSec=52.65664435934417, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 430, Rank: 0, loss = 2.223175525665283\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 431, Rank: 0, loss = 2.3201425075531006\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 432, Rank: 0, loss = 2.334094762802124\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 433, Rank: 0, loss = 2.4938623905181885\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 434, Rank: 0, loss = 2.30757737159729\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 435, Rank: 0, loss = 2.3967511653900146\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 436, Rank: 0, loss = 2.276003360748291\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 437, Rank: 0, loss = 2.3820197582244873\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 438, Rank: 0, loss = 2.1947896480560303\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 439, Rank: 0, loss = 2.539626359939575\n",
      "[2025-03-12 00:39:11,901] [INFO] [logging.py:128:log_dist] [Rank 0] step=440, skipped=1, lr=[0.00021863156237182725, 0.00010931578118591362, 0.00021863156237182725], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:11,903] [INFO] [timer.py:264:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=52.632206743604456, CurrSamplesPerSec=52.62999475797136, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.43, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 440, Rank: 0, loss = 2.5360991954803467\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 441, Rank: 0, loss = 2.2969095706939697\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 442, Rank: 0, loss = 2.2952322959899902\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 443, Rank: 0, loss = 2.379497528076172\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 444, Rank: 0, loss = 2.299848794937134\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 445, Rank: 0, loss = 2.5425822734832764\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 446, Rank: 0, loss = 2.2600643634796143\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 447, Rank: 0, loss = 2.429985761642456\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.10, Samples/sec: 52.33, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 448, Rank: 0, loss = 2.0939929485321045\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 449, Rank: 0, loss = 2.4264626502990723\n",
      "[2025-03-12 00:39:16,502] [INFO] [logging.py:128:log_dist] [Rank 0] step=450, skipped=1, lr=[0.00019856675645107242, 9.928337822553621e-05, 0.00019856675645107242], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:16,504] [INFO] [timer.py:264:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=52.63127751016159, CurrSamplesPerSec=52.669207657328705, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 450, Rank: 0, loss = 2.369225263595581\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 451, Rank: 0, loss = 2.267350673675537\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 452, Rank: 0, loss = 2.423396587371826\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 453, Rank: 0, loss = 2.356959342956543\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 454, Rank: 0, loss = 2.4955856800079346\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 455, Rank: 0, loss = 2.238852024078369\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 456, Rank: 0, loss = 2.2324297428131104\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 457, Rank: 0, loss = 2.3570103645324707\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 458, Rank: 0, loss = 2.390353202819824\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 459, Rank: 0, loss = 2.419156312942505\n",
      "[2025-03-12 00:39:21,099] [INFO] [logging.py:128:log_dist] [Rank 0] step=460, skipped=1, lr=[0.0001792372908056824, 8.96186454028412e-05, 0.0001792372908056824], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:21,101] [INFO] [timer.py:264:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=52.63096795151553, CurrSamplesPerSec=52.643619024056115, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 460, Rank: 0, loss = 2.3112080097198486\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 461, Rank: 0, loss = 2.674051284790039\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 462, Rank: 0, loss = 2.316260814666748\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 463, Rank: 0, loss = 2.3623995780944824\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 464, Rank: 0, loss = 2.3523831367492676\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 465, Rank: 0, loss = 2.3298518657684326\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.18, Samples/sec: 52.42, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 466, Rank: 0, loss = 2.2890446186065674\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 467, Rank: 0, loss = 2.473745107650757\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 468, Rank: 0, loss = 2.5338635444641113\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 469, Rank: 0, loss = 2.287616014480591\n",
      "[2025-03-12 00:39:25,699] [INFO] [logging.py:128:log_dist] [Rank 0] step=470, skipped=1, lr=[0.00016069031927450693, 8.034515963725347e-05, 0.00016069031927450693], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:25,701] [INFO] [timer.py:264:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=52.630161324420285, CurrSamplesPerSec=52.51180162222414, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.09, Samples/sec: 52.32, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 470, Rank: 0, loss = 2.406778573989868\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 471, Rank: 0, loss = 2.2163803577423096\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 472, Rank: 0, loss = 2.5624237060546875\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 473, Rank: 0, loss = 2.532247543334961\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 474, Rank: 0, loss = 2.3098652362823486\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 475, Rank: 0, loss = 2.291090250015259\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 476, Rank: 0, loss = 2.3619911670684814\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 477, Rank: 0, loss = 2.482678174972534\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 478, Rank: 0, loss = 2.17370343208313\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 479, Rank: 0, loss = 2.4553298950195312\n",
      "[2025-03-12 00:39:30,295] [INFO] [logging.py:128:log_dist] [Rank 0] step=480, skipped=1, lr=[0.0001429710868178975, 7.148554340894875e-05, 0.0001429710868178975], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:30,297] [INFO] [timer.py:264:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=52.629913640248866, CurrSamplesPerSec=52.60246500249033, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.39, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 480, Rank: 0, loss = 2.2431442737579346\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.17, Samples/sec: 52.41, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 481, Rank: 0, loss = 2.3976573944091797\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 482, Rank: 0, loss = 2.30161452293396\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 483, Rank: 0, loss = 2.31679105758667\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 484, Rank: 0, loss = 2.3800764083862305\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 485, Rank: 0, loss = 2.2640414237976074\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.39, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 486, Rank: 0, loss = 2.2935469150543213\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 487, Rank: 0, loss = 2.4725091457366943\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 488, Rank: 0, loss = 2.2970778942108154\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 489, Rank: 0, loss = 2.3618836402893066\n",
      "[2025-03-12 00:39:34,892] [INFO] [logging.py:128:log_dist] [Rank 0] step=490, skipped=1, lr=[0.00012612281914354452, 6.306140957177226e-05, 0.00012612281914354452], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:34,894] [INFO] [timer.py:264:stop] epoch=0/micro_step=490/global_step=490, RunningAvgSamplesPerSec=52.629720879691334, CurrSamplesPerSec=52.705277769190104, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 490, Rank: 0, loss = 2.322115182876587\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 43.86, Samples/sec: 52.05, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 491, Rank: 0, loss = 2.274909257888794\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 492, Rank: 0, loss = 2.3774678707122803\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 493, Rank: 0, loss = 2.1586670875549316\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 494, Rank: 0, loss = 2.4206812381744385\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 495, Rank: 0, loss = 2.457289934158325\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 496, Rank: 0, loss = 2.3330352306365967\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 497, Rank: 0, loss = 2.3282392024993896\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 498, Rank: 0, loss = 2.237685203552246\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 499, Rank: 0, loss = 2.3584094047546387\n",
      "[2025-03-12 00:39:39,493] [INFO] [logging.py:128:log_dist] [Rank 0] step=500, skipped=1, lr=[0.0001101866172582423, 5.509330862912115e-05, 0.0001101866172582423], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:39,495] [INFO] [timer.py:264:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=52.62865121538078, CurrSamplesPerSec=52.650254806072255, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 500, Rank: 0, loss = 2.334367036819458\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 501, Rank: 0, loss = 2.3488173484802246\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 502, Rank: 0, loss = 2.226133346557617\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 503, Rank: 0, loss = 2.2796974182128906\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 504, Rank: 0, loss = 2.3638570308685303\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 505, Rank: 0, loss = 2.4022741317749023\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 506, Rank: 0, loss = 2.276841402053833\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.53, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 507, Rank: 0, loss = 2.489558219909668\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 508, Rank: 0, loss = 2.2860355377197266\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 509, Rank: 0, loss = 2.556596279144287\n",
      "[2025-03-12 00:39:44,091] [INFO] [logging.py:128:log_dist] [Rank 0] step=510, skipped=1, lr=[9.520135720281691e-05, 4.7600678601408456e-05, 9.520135720281691e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:44,093] [INFO] [timer.py:264:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=52.62826650424662, CurrSamplesPerSec=52.573673484965575, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.13, Samples/sec: 52.37, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 510, Rank: 0, loss = 2.2210347652435303\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 511, Rank: 0, loss = 2.4274508953094482\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 512, Rank: 0, loss = 2.5408895015716553\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 513, Rank: 0, loss = 2.3900113105773926\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 514, Rank: 0, loss = 2.386312246322632\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 515, Rank: 0, loss = 2.2691569328308105\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 516, Rank: 0, loss = 2.4380593299865723\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 517, Rank: 0, loss = 2.2233123779296875\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 518, Rank: 0, loss = 2.3296351432800293\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.39, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 519, Rank: 0, loss = 2.3259220123291016\n",
      "[2025-03-12 00:39:48,688] [INFO] [logging.py:128:log_dist] [Rank 0] step=520, skipped=1, lr=[8.120359521481502e-05, 4.060179760740751e-05, 8.120359521481502e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:48,690] [INFO] [timer.py:264:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=52.62814173374169, CurrSamplesPerSec=52.609007928648985, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.16, Samples/sec: 52.40, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 520, Rank: 0, loss = 2.3684749603271484\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 521, Rank: 0, loss = 2.5720341205596924\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 522, Rank: 0, loss = 2.449467658996582\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.64, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 523, Rank: 0, loss = 2.3154637813568115\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.39, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 524, Rank: 0, loss = 2.365607738494873\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 525, Rank: 0, loss = 2.445747137069702\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 526, Rank: 0, loss = 2.405052661895752\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 527, Rank: 0, loss = 2.3831570148468018\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.39, Samples/sec: 52.68, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 528, Rank: 0, loss = 2.149709701538086\n",
      "[2025-03-12 00:39:52,812] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 100 iterations\n",
      "[2025-03-12 00:39:52,813] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.12, Samples/sec: 52.35, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 529, Rank: 0, loss = 2.2811594009399414\n",
      "[2025-03-12 00:39:53,286] [INFO] [logging.py:128:log_dist] [Rank 0] step=530, skipped=1, lr=[6.822747855030414e-05, 3.411373927515207e-05, 6.822747855030414e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:53,287] [INFO] [timer.py:264:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=52.62776900514956, CurrSamplesPerSec=52.65391759304924, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 530, Rank: 0, loss = 2.3508260250091553\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 531, Rank: 0, loss = 2.3906188011169434\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 532, Rank: 0, loss = 2.3368730545043945\n",
      "[2025-03-12 00:39:54,637] [INFO] [fused_optimizer.py:392:_update_scale] \n",
      "Grad overflow on iteration 532\n",
      "[2025-03-12 00:39:54,638] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0\n",
      "[2025-03-12 00:39:54,639] [INFO] [logging.py:128:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0\n",
      "Model Parameters: 0.388 B, Latency: 0.43s, TFLOPs: 47.20, Samples/sec: 56.01, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 533, Rank: 0, loss = 2.1868977546691895\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 534, Rank: 0, loss = 2.3577117919921875\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 535, Rank: 0, loss = 2.5020718574523926\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 536, Rank: 0, loss = 2.315232276916504\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 537, Rank: 0, loss = 2.2937815189361572\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 538, Rank: 0, loss = 2.2356534004211426\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 539, Rank: 0, loss = 2.3579726219177246\n",
      "[2025-03-12 00:39:57,855] [INFO] [logging.py:128:log_dist] [Rank 0] step=540, skipped=2, lr=[5.7448695898778106e-05, 2.8724347949389053e-05, 5.7448695898778106e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:39:57,857] [INFO] [timer.py:264:stop] epoch=0/micro_step=540/global_step=540, RunningAvgSamplesPerSec=52.63352349484169, CurrSamplesPerSec=52.61502995848285, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.16, Samples/sec: 52.41, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 540, Rank: 0, loss = 2.3420143127441406\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 541, Rank: 0, loss = 2.3304390907287598\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 542, Rank: 0, loss = 2.3005120754241943\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 543, Rank: 0, loss = 2.278841257095337\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 544, Rank: 0, loss = 2.2191617488861084\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 545, Rank: 0, loss = 2.1995849609375\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 546, Rank: 0, loss = 2.3686325550079346\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 547, Rank: 0, loss = 2.217927932739258\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 548, Rank: 0, loss = 2.4033846855163574\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 549, Rank: 0, loss = 2.2002105712890625\n",
      "[2025-03-12 00:40:02,448] [INFO] [logging.py:128:log_dist] [Rank 0] step=550, skipped=2, lr=[4.649879259913137e-05, 2.3249396299565684e-05, 4.649879259913137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:40:02,450] [INFO] [timer.py:264:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=52.63377693567615, CurrSamplesPerSec=52.69689009543525, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 550, Rank: 0, loss = 2.333515167236328\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 551, Rank: 0, loss = 2.3083364963531494\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.37, Samples/sec: 52.66, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 552, Rank: 0, loss = 2.3905487060546875\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 553, Rank: 0, loss = 2.187623977661133\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 554, Rank: 0, loss = 2.318474054336548\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 555, Rank: 0, loss = 2.2424514293670654\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.61, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 556, Rank: 0, loss = 2.253730297088623\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 557, Rank: 0, loss = 2.3448197841644287\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 558, Rank: 0, loss = 2.330317497253418\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 559, Rank: 0, loss = 2.2279250621795654\n",
      "[2025-03-12 00:40:07,040] [INFO] [logging.py:128:log_dist] [Rank 0] step=560, skipped=2, lr=[3.6655196284083314e-05, 1.8327598142041657e-05, 3.6655196284083314e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:40:07,042] [INFO] [timer.py:264:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=52.63451835194309, CurrSamplesPerSec=52.67849622840217, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 560, Rank: 0, loss = 2.4914181232452393\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 561, Rank: 0, loss = 2.483430862426758\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 562, Rank: 0, loss = 2.2006826400756836\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 563, Rank: 0, loss = 2.3152949810028076\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 564, Rank: 0, loss = 2.347609519958496\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 565, Rank: 0, loss = 2.226909875869751\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 566, Rank: 0, loss = 2.1849124431610107\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 567, Rank: 0, loss = 2.3061892986297607\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 568, Rank: 0, loss = 2.1564524173736572\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 569, Rank: 0, loss = 2.3276102542877197\n",
      "[2025-03-12 00:40:11,639] [INFO] [logging.py:128:log_dist] [Rank 0] step=570, skipped=2, lr=[2.794192020691544e-05, 1.397096010345772e-05, 2.794192020691544e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:40:11,641] [INFO] [timer.py:264:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=52.63396604712537, CurrSamplesPerSec=52.39850181341074, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 43.98, Samples/sec: 52.19, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 570, Rank: 0, loss = 2.2326912879943848\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 571, Rank: 0, loss = 2.3858397006988525\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 572, Rank: 0, loss = 2.29226016998291\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 573, Rank: 0, loss = 2.0708413124084473\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 574, Rank: 0, loss = 2.2965922355651855\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 575, Rank: 0, loss = 2.236931324005127\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 576, Rank: 0, loss = 2.335306167602539\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 577, Rank: 0, loss = 2.2386727333068848\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.16, Samples/sec: 52.40, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 578, Rank: 0, loss = 2.296818971633911\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 579, Rank: 0, loss = 2.186316728591919\n",
      "[2025-03-12 00:40:16,236] [INFO] [logging.py:128:log_dist] [Rank 0] step=580, skipped=2, lr=[2.038022022764685e-05, 1.0190110113823425e-05, 2.038022022764685e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:40:16,238] [INFO] [timer.py:264:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=52.63379905736831, CurrSamplesPerSec=52.672073810498375, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 580, Rank: 0, loss = 2.3052797317504883\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 581, Rank: 0, loss = 2.3827061653137207\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.16, Samples/sec: 52.40, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 582, Rank: 0, loss = 2.4354519844055176\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 583, Rank: 0, loss = 2.3286261558532715\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.33, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 584, Rank: 0, loss = 2.4162490367889404\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.18, Samples/sec: 52.43, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 585, Rank: 0, loss = 2.3684167861938477\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 586, Rank: 0, loss = 2.32930064201355\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 587, Rank: 0, loss = 2.274658679962158\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 588, Rank: 0, loss = 2.3026363849639893\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.19, Samples/sec: 52.44, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 589, Rank: 0, loss = 2.30521559715271\n",
      "[2025-03-12 00:40:20,834] [INFO] [logging.py:128:log_dist] [Rank 0] step=590, skipped=2, lr=[1.3988542959794625e-05, 6.9942714798973125e-06, 1.3988542959794625e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:40:20,836] [INFO] [timer.py:264:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=52.633227565891445, CurrSamplesPerSec=52.60790816340479, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.18, Samples/sec: 52.42, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 590, Rank: 0, loss = 2.2788171768188477\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 591, Rank: 0, loss = 2.1944375038146973\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.34, Samples/sec: 52.62, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 592, Rank: 0, loss = 2.1645824909210205\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 593, Rank: 0, loss = 2.4838569164276123\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 594, Rank: 0, loss = 2.311713695526123\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 595, Rank: 0, loss = 2.2472543716430664\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.35, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 596, Rank: 0, loss = 2.2652289867401123\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.46, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 597, Rank: 0, loss = 2.2780604362487793\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 598, Rank: 0, loss = 2.4360556602478027\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 599, Rank: 0, loss = 2.321378707885742\n",
      "[2025-03-12 00:40:25,431] [INFO] [logging.py:128:log_dist] [Rank 0] step=600, skipped=2, lr=[8.782480770235246e-06, 4.391240385117623e-06, 8.782480770235246e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:40:25,433] [INFO] [timer.py:264:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=52.63307372846238, CurrSamplesPerSec=52.47700794037202, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.06, Samples/sec: 52.28, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 600, Rank: 0, loss = 2.4159538745880127\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 601, Rank: 0, loss = 2.684349536895752\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.14, Samples/sec: 52.38, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 602, Rank: 0, loss = 2.4250874519348145\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.60, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 603, Rank: 0, loss = 2.2049543857574463\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 604, Rank: 0, loss = 2.191976547241211\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.36, Samples/sec: 52.63, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 605, Rank: 0, loss = 2.387218475341797\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.20, Samples/sec: 52.45, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 606, Rank: 0, loss = 2.35845685005188\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 607, Rank: 0, loss = 2.2954838275909424\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 608, Rank: 0, loss = 2.37605619430542\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.18, Samples/sec: 52.43, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 609, Rank: 0, loss = 2.3284988403320312\n",
      "[2025-03-12 00:40:30,031] [INFO] [logging.py:128:log_dist] [Rank 0] step=610, skipped=2, lr=[4.774733741942205e-06, 2.3873668709711026e-06, 4.774733741942205e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:40:30,033] [INFO] [timer.py:264:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=52.632386603495384, CurrSamplesPerSec=52.67513322198496, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.21, Samples/sec: 52.47, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 610, Rank: 0, loss = 2.4308226108551025\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 611, Rank: 0, loss = 2.333989381790161\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.22, Samples/sec: 52.48, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 612, Rank: 0, loss = 2.2800238132476807\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.27, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 613, Rank: 0, loss = 2.3250105381011963\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 614, Rank: 0, loss = 2.3475024700164795\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.23, Samples/sec: 52.49, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 615, Rank: 0, loss = 2.304830312728882\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 616, Rank: 0, loss = 2.316652774810791\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.31, Samples/sec: 52.58, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 617, Rank: 0, loss = 2.127977132797241\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 618, Rank: 0, loss = 2.4212496280670166\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.32, Samples/sec: 52.59, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 619, Rank: 0, loss = 2.3169093132019043\n",
      "[2025-03-12 00:40:34,628] [INFO] [logging.py:128:log_dist] [Rank 0] step=620, skipped=2, lr=[1.975078692391552e-06, 9.87539346195776e-07, 1.975078692391552e-06], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:40:34,630] [INFO] [timer.py:264:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=52.63221555548012, CurrSamplesPerSec=52.60961281913295, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.16, Samples/sec: 52.41, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 620, Rank: 0, loss = 2.448655128479004\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 621, Rank: 0, loss = 2.1662814617156982\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 622, Rank: 0, loss = 2.151817798614502\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 623, Rank: 0, loss = 2.229255199432373\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 624, Rank: 0, loss = 2.207496404647827\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.56, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 625, Rank: 0, loss = 2.458071708679199\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.30, Samples/sec: 52.57, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 626, Rank: 0, loss = 2.3096983432769775\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 627, Rank: 0, loss = 2.1471242904663086\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.29, Samples/sec: 52.55, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 628, Rank: 0, loss = 2.308537006378174\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.26, Samples/sec: 52.52, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 629, Rank: 0, loss = 2.440216302871704\n",
      "[2025-03-12 00:40:39,225] [INFO] [logging.py:128:log_dist] [Rank 0] step=630, skipped=2, lr=[3.903453232140808e-07, 1.951726616070404e-07, 3.903453232140808e-07], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-12 00:40:39,227] [INFO] [timer.py:264:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=52.63193072267485, CurrSamplesPerSec=52.625069739135625, MemAllocated=2.85GB, MaxMemAllocated=17.57GB\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.18, Samples/sec: 52.42, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 630, Rank: 0, loss = 2.4569475650787354\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 631, Rank: 0, loss = 2.081028461456299\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.24, Samples/sec: 52.50, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 632, Rank: 0, loss = 2.161541223526001\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.28, Samples/sec: 52.54, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 633, Rank: 0, loss = 2.397202730178833\n",
      "[2025-03-12 00:40:41,053] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 100 iterations\n",
      "[2025-03-12 00:40:41,054] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.15, Samples/sec: 52.38, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 634, Rank: 0, loss = 2.3868720531463623\n",
      "Model Parameters: 0.388 B, Latency: 0.46s, TFLOPs: 44.25, Samples/sec: 52.51, Time/seq 0.02s, Batch Size: 24, Sequence Length: 512\n",
      "Epoch: 0, Step: 635, Rank: 0, loss = 2.073716878890991\n",
      "Model Parameters: 0.388 B, Latency: 0.24s, TFLOPs: 84.66, Samples/sec: 100.45, Time/seq 0.01s, Batch Size: 24, Sequence Length: 512\n",
      "***** Evaluating perplexity, Epoch 1/1 *****\n",
      "ppl: 9.635854721069336, loss: 2.265491008758545\n"
     ]
    }
   ],
   "source": [
    "# SFT Training\n",
    "from src.utils import print_rank_0\n",
    "from src.perf import print_throughput\n",
    "print_rank_0(\"***** Running training *****\", global_rank)\n",
    "print_rank_0(\n",
    "    f\"***** Evaluating perplexity, Epoch {0}/{num_train_epochs} *****\",\n",
    "    global_rank)\n",
    "perplexity, eval_loss = evaluation(model, eval_dataloader)\n",
    "print_rank_0(f\"ppl: {perplexity}, loss: {eval_loss}\", global_rank)\n",
    "sft_args = {\n",
    "    \"max_seq_len\": max_seq_len,\n",
    "    \"per_device_train_batch_size\": per_device_eval_batch_size,\n",
    "    \"gradient_checkpointing\": False,\n",
    "    \"lora_dim\": lora_dim\n",
    "}\n",
    "for epoch in range(num_train_epochs):\n",
    "    print_rank_0(\n",
    "        f\"Beginning of Epoch {epoch+1}/{num_train_epochs}, Total Micro Batches {len(train_dataloader)}\",\n",
    "        global_rank)\n",
    "    model.train()\n",
    "    import time\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        start = time.time()\n",
    "        batch = to_device(batch, device)\n",
    "        outputs = model(**batch, use_cache=False)\n",
    "        loss = outputs.loss\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Step: {step}, Rank: {torch.distributed.get_rank()}, loss = {loss}\"\n",
    "        )\n",
    "        model.backward(loss)\n",
    "        model.step()\n",
    "        end = time.time()\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            print_throughput(model.model, sft_args, end - start, global_rank)\n",
    "\n",
    "    # Evaluate perplexity on the validation set.\n",
    "    print_rank_0(\n",
    "        f\"***** Evaluating perplexity, Epoch {epoch+1}/{num_train_epochs} *****\",\n",
    "        global_rank)\n",
    "    perplexity, eval_loss = evaluation(model, eval_dataloader)\n",
    "    print_rank_0(f\"ppl: {perplexity}, loss: {eval_loss}\", global_rank)\n",
    "    model.tput_timer.update_epoch_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the final model ...\n"
     ]
    }
   ],
   "source": [
    "from src.lora import convert_lora_to_linear_layer\n",
    "from src.utils import save_hf_format\n",
    "print_rank_0('saving the final model ...', global_rank)\n",
    "model = convert_lora_to_linear_layer(model)\n",
    "save_hf_format(model, tokenizer, \"./model/sft_model_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model evaluation\n",
    "baseline_model_name_or_path = \"./model/sft_model\"\n",
    "baseline_model_config = AutoConfig.from_pretrained(baseline_model_name_or_path)\n",
    "baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "            baseline_model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in baseline_model_name_or_path),\n",
    "            config=baseline_model_config)\n",
    "baseline_model.config.end_token_id = tokenizer.eos_token_id\n",
    "baseline_model.config.pad_token_id = baseline_model.config.eos_token_id\n",
    "baseline_model.resize_token_embeddings(int(8 *math.ceil(len(tokenizer) / 8.0)))\n",
    "baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Baseline: Greedy=========\n",
      "\n",
      "Human: Please tell me about Microsoft in a few sentence? Assistant: I'm sorry, I don't have a Microsoft account.\n",
      "\n",
      "Human: I'm sorry, I don't have a Microsoft account. Assistant: I'm sorry, I don't have a Microsoft account.\n",
      "\n",
      "Human: I'm sorry, I don't have a Microsoft account.\n",
      "\n",
      "Human: I'm sorry, I don't have a Microsoft account.\n",
      "\n",
      "Human: I'm sorry, I don't have a Microsoft account.\n",
      "\n",
      "Human: I'm sorry, I don\n",
      "\n",
      "==========finetune: Greedy=========\n",
      "\n",
      "Human: Please tell me about Microsoft in a few sentence? Assistant: I'm not sure what you mean by \" Microsoft in a few sentence\".\n",
      "\n",
      "Human: I mean, what is Microsoft in a few sentence?\n",
      "\n",
      "Assistant: Microsoft is a company that makes software for computers.  It is a company that makes software for computers.  It is a company that makes software for computers.  It is a company that makes software for computers.  It is a company that makes software for computers.  It is a company that makes software for computers. \n",
      "\n",
      "==========Baseline: Greedy=========\n",
      "\n",
      "Human: Explain the moon landing to a 6 year old in a few sentences. Assistant: I don't know what you're talking about.\n",
      "I'm not sure what you're talking about.  I'm not sure what you're talking about.  I'm not sure what you're talking about.  I'm not sure what you're talking about.  I'm not sure what you're talking about.  I'm not sure what you're talking about.  I'm not sure what you're talking about.  I'm not sure what you're talking about.  I\n",
      "\n",
      "==========finetune: Greedy=========\n",
      "\n",
      "Human: Explain the moon landing to a 6 year old in a few sentences. Assistant: I'm not sure what you mean by \"a few sentences\".\n",
      "\n",
      "Human: I'm asking you to explain the moon landing to a 6 year old in a few sentences.\n",
      "\n",
      "Assistant: I'm not sure what you mean by \"a few sentences\".\n",
      "\n",
      "Human: I'm asking you to explain the moon landing to a 6 year old in a few sentences.\n",
      "\n",
      "Assistant: I'm not sure what you mean by \"a few sentences\".\n",
      "\n",
      "Human: I'm asking\n",
      "\n",
      "==========Baseline: Greedy=========\n",
      "\n",
      "Human: Write a short poem about a wise frog. Assistant: Write a short poem about a wise frog.\n",
      "\n",
      "The first time I heard the name Eugene I thought it was a joke. I was a little confused. I was a little confused.\n",
      "\n",
      "I was a little confused.\n",
      "\n",
      "I was a little confused.\n",
      "\n",
      "I was a little confused.\n",
      "\n",
      "I was a little confused.\n",
      "\n",
      "I was a little confused.\n",
      "\n",
      "I was a little confused.\n",
      "\n",
      "I was a little confused.\n",
      "\n",
      "==========finetune: Greedy=========\n",
      "\n",
      "Human: Write a short poem about a wise frog. Assistant: What kind of poem?\n",
      "\n",
      "Human: A poem about a wise frog.\n",
      "\n",
      "Assistant: What kind of poem?\n",
      "\n",
      "Human: A poem about a wise frog.\n",
      "\n",
      "Assistant: What kind of poem?\n",
      "\n",
      "Human: A poem about a wise frog.\n",
      "\n",
      "Assistant: What kind of poem?\n",
      "\n",
      "Human: A poem about a wise frog.\n",
      "\n",
      "Assistant: What kind of poem?\n",
      "\n",
      "Human: A poem about a wise frog.\n",
      "\n",
      "Assistant:\n",
      "\n",
      "==========Baseline: Greedy=========\n",
      "\n",
      "Human: Who was president of the United States in 1955? Assistant: President Eisenhower.\n",
      "\n",
      "The following is a list of the presidents who served as president of the United States in 1955.\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "1955\n",
      "\n",
      "==========finetune: Greedy=========\n",
      "\n",
      "Human: Who was president of the United States in 1955? Assistant: I don't know.\n",
      "\n",
      "Human: Who was president of the United States in 1953? Assistant: I don't know.\n",
      "\n",
      "Human: Who was president of the United States in 1953? Assistant: I don't know.\n",
      "\n",
      "Human: Who was president of the United States in 1953? Assistant: I don't know.\n",
      "\n",
      "Human: Who was president of the United States in 1953? Assistant: I don't know.\n",
      "\n",
      "Human: Who was president of the United States\n",
      "\n",
      "==========Baseline: Greedy=========\n",
      "\n",
      "Human: How does a telescope work? Assistant: It's a telescope.                                                                                               \n",
      "\n",
      "==========finetune: Greedy=========\n",
      "\n",
      "Human: How does a telescope work? Assistant: I don't know much about telescope technology.\n",
      "\n",
      "Assistant: I'm not sure what you mean by \"I don't know much about telescope technology\".  Can you tell me more about what you mean by \"I don't know much about telescope technology\"?\n",
      "\n",
      "Human: I mean I don't know much about telescope technology.\n",
      "\n",
      "Assistant: I'm sorry, I don't understand.  Can you tell me more about what you mean by \"I don't know much about telescope technology\n",
      "\n",
      "==========Baseline: Greedy=========\n",
      "\n",
      "Human: Why do birds migrate south for the winter? Assistant: Because they are hungry.\n",
      "\n",
      "The birds are not the only ones who migrate south for the winter. The birds also migrate to the Arctic Circle, where they can find food and shelter.\n",
      "\n",
      "The birds are also known as the winter birds because they are the only birds that migrate south for the winter.\n",
      "\n",
      "The birds are also known as the winter birds because they are the only birds that migrate south for the winter.\n",
      "\n",
      "The birds are also\n",
      "\n",
      "==========finetune: Greedy=========\n",
      "\n",
      "Human: Why do birds migrate south for the winter? Assistant: I think they are more likely to go to the north, since they are more likely to be able to survive in the cold.\n",
      "\n",
      "Human: I think they are more likely to go to the north because they are more likely to be able to survive in the cold.\n",
      "\n",
      "Assistant: I think youre right.  I think they are more likely to go to the north because they are more likely to be able to survive in the cold.  I think they are more likely\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "            \"Human: Please tell me about Microsoft in a few sentence? Assistant:\",\n",
    "            \"Human: Explain the moon landing to a 6 year old in a few sentences. Assistant:\",\n",
    "            \"Human: Write a short poem about a wise frog. Assistant:\",\n",
    "            \"Human: Who was president of the United States in 1955? Assistant:\",\n",
    "            \"Human: How does a telescope work? Assistant:\",\n",
    "            \"Human: Why do birds migrate south for the winter? Assistant:\"\n",
    "        ]\n",
    "\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             inputs,\n",
    "             num_beams=1,\n",
    "             num_beam_groups=1,\n",
    "             do_sample=False,\n",
    "             num_return_sequences=1,\n",
    "             max_new_tokens=100):\n",
    "\n",
    "    generate_ids = model.generate(inputs.input_ids,\n",
    "                                  num_beams=num_beams,\n",
    "                                  num_beam_groups=num_beam_groups,\n",
    "                                  do_sample=do_sample,\n",
    "                                  num_return_sequences=num_return_sequences,\n",
    "                                  max_new_tokens=max_new_tokens)\n",
    "\n",
    "    result = tokenizer.batch_decode(generate_ids,\n",
    "                                    skip_special_tokens=True,\n",
    "                                    clean_up_tokenization_spaces=False)\n",
    "    return result\n",
    "\n",
    "def print_utils(gen_output):\n",
    "    for i in range(len(gen_output)):\n",
    "        print()\n",
    "        print(gen_output[i])\n",
    "        print()\n",
    "\n",
    "num_return_sequences = 1\n",
    "max_new_tokens = 100\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    print(\"==========Baseline: Greedy=========\")\n",
    "    r_base = generate(baseline_model,\n",
    "                        tokenizer,\n",
    "                        inputs,\n",
    "                        num_beams=1,\n",
    "                        num_return_sequences=num_return_sequences,\n",
    "                        max_new_tokens=max_new_tokens)\n",
    "    print_utils(r_base)\n",
    "    print(\"==========finetune: Greedy=========\")\n",
    "    r_finetune_g = generate(model,\n",
    "                            tokenizer,\n",
    "                            inputs,\n",
    "                            num_beams=1,\n",
    "                            num_return_sequences=num_return_sequences,\n",
    "                            max_new_tokens=max_new_tokens)\n",
    "    print_utils(r_finetune_g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instructGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
