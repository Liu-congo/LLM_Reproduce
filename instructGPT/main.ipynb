{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft model download\n",
    "# execute the following commands in shell or you will meet network problems\n",
    "!export HF_ENDPOINT=https://hf-mirror.com\n",
    "!huggingface-cli download --resume-download facebook/opt-350m --local-dir ./model/sft_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset download\n",
    "!huggingface-cli download --repo-type dataset --resume-download Dahoas/rm-static --local-dir ./data/rm_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm model download\n",
    "!huggingface-cli download --resume-download facebook/opt-125m --local-dir ./model/rm_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: SFT training\n",
    "adapting from https://github.com/deepspeedai/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-11 18:36:13,565] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/instructGPT/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/root/miniconda3/envs/instructGPT/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/root/miniconda3/envs/instructGPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the device\n",
    "import torch\n",
    "from deepspeed import get_accelerator\n",
    "device = torch.device(get_accelerator().device_name())\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-11 18:36:20,912] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-03-11 18:36:20,914] [INFO] [comm.py:673:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2025-03-11 18:36:21,053] [INFO] [comm.py:728:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.17.0.9, master_port=29500\n",
      "[2025-03-11 18:36:21,055] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W311 18:36:21.654211060 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n"
     ]
    }
   ],
   "source": [
    "# initialize deepspeed config\n",
    "from src.utils import get_train_ds_config\n",
    "import deepspeed\n",
    "deepspeed.init_distributed()\n",
    "global_rank = torch.distributed.get_rank()\n",
    "ds_config = get_train_ds_config(offload=False,\n",
    "                                    dtype='fp16',\n",
    "                                    stage=0,\n",
    "                                    enable_tensorboard=True,\n",
    "                                    tb_path=\"step1_tensorboard\",\n",
    "                                    tb_name=\"step1_model\")\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "ds_config['train_micro_batch_size_per_gpu'] = per_device_train_batch_size\n",
    "ds_config['train_batch_size'] = per_device_train_batch_size * torch.distributed.get_world_size() * gradient_accumulation_steps\n",
    "torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='./model/sft_model', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "model_name_or_path = \"./model/sft_model\"\n",
    "model_json = os.path.join(model_name_or_path, \"config.json\")\n",
    "if os.path.exists(model_json):\n",
    "    model_json_file = json.load(open(model_json))\n",
    "    model_name = model_json_file.get(\"_name_or_path\",\n",
    "                                        model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, fast_tokenizer=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'right'\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize sft model\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "import math\n",
    "model_name_or_path = \"./model/sft_model\"\n",
    "model_config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "            config=model_config)\n",
    "model.config.end_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.resize_token_embeddings(int(8 *math.ceil(len(tokenizer) / 8.0)))\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): LinearLayer_LoRA(\n",
       "              (lora_dropout): Identity()\n",
       "            )\n",
       "            (v_proj): LinearLayer_LoRA(\n",
       "              (lora_dropout): Identity()\n",
       "            )\n",
       "            (q_proj): LinearLayer_LoRA(\n",
       "              (lora_dropout): Identity()\n",
       "            )\n",
       "            (out_proj): LinearLayer_LoRA(\n",
       "              (lora_dropout): Identity()\n",
       "            )\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): LinearLayer_LoRA(\n",
       "            (lora_dropout): Identity()\n",
       "          )\n",
       "          (fc2): LinearLayer_LoRA(\n",
       "            (lora_dropout): Identity()\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert model to lora form for efficient sft\n",
    "from src.lora import convert_linear_layer_to_lora\n",
    "lora_module_name = \"decoder.layers.\"\n",
    "lora_dim = 128\n",
    "model = convert_linear_layer_to_lora(model, lora_module_name, lora_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating prompt dataset ['./data/rm_static'], reload=False\n",
      "Creating dataset Dahoas_rm_static for train_phase=1 size=15252\n",
      "Creating dataset Dahoas_rm_static for train_phase=1 size=1021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.data_utils.PromptDataset at 0x7f03d88fc3d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Prepare the data\n",
    "from src.data_utils import create_prompt_dataset\n",
    "train_phase = 1\n",
    "local_rank = -1\n",
    "data_path = [\"./data/rm_static\"]\n",
    "data_split = \"2,4,4\"\n",
    "data_output_path = \"./data/rm_static_processed4sft/\"\n",
    "max_seq_len = 512\n",
    "train_dataset, eval_dataset = create_prompt_dataset(\n",
    "    local_rank,\n",
    "    data_path,\n",
    "    data_split,\n",
    "    data_output_path,\n",
    "    train_phase,\n",
    "    1234,\n",
    "    tokenizer,\n",
    "    max_seq_len,\n",
    "    end_of_conversation_token=tokenizer.eos_token)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import default_data_collator\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                                  collate_fn=default_data_collator,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=per_device_train_batch_size)\n",
    "eval_dataloader = DataLoader(eval_dataset,\n",
    "                                collate_fn=default_data_collator,\n",
    "                                sampler=eval_sampler,\n",
    "                                batch_size=per_device_eval_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instructGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
