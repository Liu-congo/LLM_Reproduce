{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Finetuning a Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-17 01:18:19,623] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/instructGPT/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/root/miniconda3/envs/instructGPT/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/root/miniconda3/envs/instructGPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-17 01:18:21,982] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-03-17 01:18:21,983] [INFO] [comm.py:673:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2025-03-17 01:18:22,081] [INFO] [comm.py:728:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.17.0.7, master_port=29500\n",
      "[2025-03-17 01:18:22,083] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W317 01:18:22.069052848 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n"
     ]
    }
   ],
   "source": [
    "# device preparation & dsconfig setting\n",
    "import torch\n",
    "from deepspeed import get_accelerator\n",
    "from src.ds_utils import get_train_ds_config\n",
    "import deepspeed\n",
    "\n",
    "device = torch.device(get_accelerator().device_name())\n",
    "deepspeed.init_distributed()\n",
    "global_rank = torch.distributed.get_rank()\n",
    "ds_config = get_train_ds_config(offload=False,\n",
    "                                    dtype='fp16',\n",
    "                                    stage=0,\n",
    "                                    enable_tensorboard=True,\n",
    "                                    tb_path=\"step2_tensorboard\",\n",
    "                                    tb_name=\"step2_model\")\n",
    "per_device_train_batch_size = 24\n",
    "per_device_eval_batch_size = 24\n",
    "gradient_accumulation_steps = 4\n",
    "ds_config['train_micro_batch_size_per_gpu'] = per_device_train_batch_size\n",
    "ds_config['train_batch_size'] = per_device_train_batch_size * torch.distributed.get_world_size() * gradient_accumulation_steps\n",
    "torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='./model/rm_model', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "model_name_or_path = \"./model/rm_model\"\n",
    "model_json = os.path.join(model_name_or_path, \"config.json\")\n",
    "if os.path.exists(model_json):\n",
    "    model_json_file = json.load(open(model_json))\n",
    "    model_name = model_json_file.get(\"_name_or_path\",\n",
    "                                        model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, fast_tokenizer=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'right'\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting model_config.dropout to 0.0\n",
      "Setting model_config.attention_dropout to 0.0\n",
      "Setting model_config.activation_dropout to 0.0\n",
      ">Creating model from_config took 0.25955820083618164 seconds\n"
     ]
    }
   ],
   "source": [
    "# initialize sft model\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from src.utils import print_rank_0\n",
    "import math\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "dropout = 0.0\n",
    "model_config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "for key in ('dropout', 'attention_dropout', 'hidden_dropout',\n",
    "                    'activation_dropout'):\n",
    "    if hasattr(model_config, key):\n",
    "        print(f\"Setting model_config.{key} to {dropout}\")\n",
    "        setattr(model_config, key, dropout)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "            config=model_config)\n",
    "model.config.end_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.resize_token_embeddings(int(8 *math.ceil(len(tokenizer) / 8.0)))\n",
    "end = time.time()\n",
    "print_rank_0(f\">Creating model from_config took {end - start} seconds\",\n",
    "                None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardModel(\n",
       "  (v_head): Linear(in_features=768, out_features=1, bias=False)\n",
       "  (rwtransformer): OPTForCausalLM(\n",
       "    (model): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x OPTDecoderLayer(\n",
       "            (self_attn): OPTSdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the Reward Model Class & init a Critic Model from base model\n",
    "\n",
    "# Copyright (c) Microsoft Corporation.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# DeepSpeed Team\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "## Note that the following code is modified from\n",
    "## https://github.com/CarperAI/trlx/blob/main/examples/summarize_rlhf/reward_model/reward_model.py\n",
    "class RewardModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_model,\n",
    "                 tokenizer,\n",
    "                 num_padding_at_beginning=0,\n",
    "                 compute_fp32_loss=False):\n",
    "        super().__init__()\n",
    "        self.config = base_model.config\n",
    "        self.num_padding_at_beginning = num_padding_at_beginning\n",
    "\n",
    "# HERE is the key of Reward Model\n",
    "# substitute the final output layer(lm_head) with a linear projection head \n",
    "# converting the token into a scalar, representing the scores that LLM assign for the input \n",
    "\n",
    "        if hasattr(self.config, \"word_embed_proj_dim\"):\n",
    "            # `OPT` models use word_embed_proj_dim as final output\n",
    "            # https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L497\n",
    "            self.v_head = nn.Linear(self.config.word_embed_proj_dim,\n",
    "                                    1,\n",
    "                                    bias=False)\n",
    "        else:\n",
    "            # `gpt-neo(x)` models use `hidden_size` attribute names instead of `n_embd``\n",
    "            self.config.n_embd = self.config.hidden_size if hasattr(\n",
    "                self.config, \"hidden_size\") else self.config.n_embd\n",
    "            self.v_head = nn.Linear(self.config.n_embd, 1, bias=False)\n",
    "\n",
    "        self.rwtransformer = base_model\n",
    "        self.PAD_ID = tokenizer.pad_token_id\n",
    "        self.compute_fp32_loss = compute_fp32_loss\n",
    "\n",
    "    def gradient_checkpointing_enable(self):\n",
    "        self.rwtransformer.gradient_checkpointing_enable()\n",
    "\n",
    "    def gradient_checkpointing_disable(self):\n",
    "        self.rwtransformer.gradient_checkpointing_disable()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                past_key_values=None,\n",
    "                attention_mask=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                use_cache=False):\n",
    "        loss = None\n",
    "\n",
    "        if self.config.model_type == \"llama\":\n",
    "            kwargs = dict()\n",
    "        else:\n",
    "            kwargs = dict(head_mask=head_mask)\n",
    "\n",
    "        transformer_outputs = self.rwtransformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_hidden_states=True,\n",
    "            **kwargs)\n",
    "\n",
    "        hidden_states = transformer_outputs.hidden_states[-1]\n",
    "        rewards = self.v_head(hidden_states).squeeze(-1)\n",
    "        chosen_mean_scores = []\n",
    "        rejected_mean_scores = []\n",
    "\n",
    "        # Split the inputs and rewards into two parts, chosen and rejected\n",
    "        assert len(input_ids.shape) == 2\n",
    "        bs = input_ids.shape[0] // 2\n",
    "        seq_len = input_ids.shape[1]\n",
    "\n",
    "        chosen_ids = input_ids[:bs]  # bs x seq x 1\n",
    "        rejected_ids = input_ids[bs:]\n",
    "        chosen_rewards = rewards[:bs]\n",
    "        rejected_rewards = rewards[bs:]\n",
    "\n",
    "        # Compute pairwise loss. Only backprop on the different tokens before padding\n",
    "        loss = 0.\n",
    "        for i in range(bs):\n",
    "            chosen_id = chosen_ids[i]\n",
    "            rejected_id = rejected_ids[i]\n",
    "            chosen_reward = chosen_rewards[i]\n",
    "            rejected_reward = rejected_rewards[i]\n",
    "\n",
    "            c_inds = (chosen_id == self.PAD_ID).nonzero()\n",
    "            c_ind = c_inds[self.num_padding_at_beginning].item() if len(\n",
    "                c_inds\n",
    "            ) > self.num_padding_at_beginning else seq_len  # OPT model pads the first token, so we need to use the second padding token as the end of the sequence\n",
    "            check_divergence = (chosen_id != rejected_id).nonzero()\n",
    "\n",
    "            if len(check_divergence) == 0:\n",
    "                end_ind = rejected_reward.size(-1)\n",
    "                divergence_ind = end_ind - 1\n",
    "                r_ind = c_ind\n",
    "            else:\n",
    "                # Check if there is any padding otherwise take length of sequence\n",
    "                r_inds = (rejected_id == self.PAD_ID).nonzero()\n",
    "                r_ind = r_inds[self.num_padding_at_beginning].item(\n",
    "                ) if len(r_inds) > self.num_padding_at_beginning else seq_len\n",
    "                end_ind = max(c_ind, r_ind)\n",
    "                divergence_ind = check_divergence[0]\n",
    "            assert divergence_ind > 0\n",
    "            c_truncated_reward = chosen_reward[divergence_ind:end_ind]\n",
    "            r_truncated_reward = rejected_reward[divergence_ind:end_ind]\n",
    "            chosen_mean_scores.append(\n",
    "                chosen_reward[c_ind - 1])  #use the end score for reference\n",
    "            rejected_mean_scores.append(rejected_reward[r_ind - 1])\n",
    "\n",
    "            if self.compute_fp32_loss:\n",
    "                c_truncated_reward = c_truncated_reward.float()\n",
    "                r_truncated_reward = r_truncated_reward.float()\n",
    "            loss += -torch.nn.functional.logsigmoid(c_truncated_reward -\n",
    "                                                    r_truncated_reward).mean()\n",
    "\n",
    "        loss = loss / bs\n",
    "        chosen_mean_scores = torch.stack(chosen_mean_scores)\n",
    "        rejected_mean_scores = torch.stack(rejected_mean_scores)\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"chosen_mean_scores\": chosen_mean_scores,\n",
    "            \"rejected_mean_scores\": rejected_mean_scores,\n",
    "        }\n",
    "\n",
    "    def forward_value(self,\n",
    "                      input_ids=None,\n",
    "                      attention_mask=None,\n",
    "                      past_key_values=None,\n",
    "                      position_ids=None,\n",
    "                      head_mask=None,\n",
    "                      inputs_embeds=None,\n",
    "                      return_value_only=False,\n",
    "                      prompt_length=0,\n",
    "                      use_cache=False):\n",
    "\n",
    "        if self.config.model_type == \"llama\":\n",
    "            kwargs = dict()\n",
    "        else:\n",
    "            kwargs = dict(head_mask=head_mask)\n",
    "\n",
    "        transformer_outputs = self.rwtransformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_hidden_states=True,\n",
    "            **kwargs)\n",
    "        hidden_states = transformer_outputs.hidden_states[-1]\n",
    "        values = self.v_head(hidden_states).squeeze(-1)\n",
    "        if return_value_only:\n",
    "            return values\n",
    "        else:\n",
    "            # [0 0 0 0 prompt, answer, 0 0 0 0 ] for step 3, we have padding at the beginning\n",
    "            # [prompt, answer, 0, 0, 0, 0] this is normal\n",
    "            assert prompt_length > 1, \"prompt_length must be greater than 1 to help select the end score\"\n",
    "            bs = values.size(0)\n",
    "            seq_len = input_ids.shape[1]\n",
    "            chosen_end_scores = [\n",
    "            ]  # we use this name for consistency with the original forward function\n",
    "            for i in range(bs):\n",
    "                input_id = input_ids[i]\n",
    "                value = values[i]\n",
    "\n",
    "                c_inds = (input_id[prompt_length:] == self.PAD_ID).nonzero()\n",
    "                # here we only use the answer part of the sequence so we do not need to care about the padding at the beginning\n",
    "                c_ind = c_inds[0].item() + prompt_length if len(\n",
    "                    c_inds) > 0 else seq_len\n",
    "                chosen_end_scores.append(value[c_ind - 1])\n",
    "            return {\n",
    "                \"values\": values,\n",
    "                \"chosen_end_scores\": torch.stack(chosen_end_scores),\n",
    "            }\n",
    "        \n",
    "nums_padding_at_beginning = 1\n",
    "compute_fp32_loss = False\n",
    "critic_model = RewardModel(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        num_padding_at_beginning=nums_padding_at_beginning,\n",
    "        compute_fp32_loss=False)\n",
    "critic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.data_utils.PromptDataset at 0x7fd6c1e5bf70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Prepare the data\n",
    "from src.data_utils import create_prompt_dataset\n",
    "train_phase = 2\n",
    "local_rank = -1\n",
    "data_path = [\"./data/rm_static\"]\n",
    "data_split = \"2,4,4\"\n",
    "data_output_path = \"./data/rm_static_processed4rm/\"\n",
    "max_seq_len = 512\n",
    "train_dataset, eval_dataset = create_prompt_dataset(\n",
    "    local_rank,\n",
    "    data_path,\n",
    "    data_split,\n",
    "    data_output_path,\n",
    "    train_phase,\n",
    "    1234,\n",
    "    tokenizer,\n",
    "    max_seq_len)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders creation\n",
    "from src.data_utils import DataCollatorReward\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "data_collator = DataCollatorReward()\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                                collate_fn=data_collator,\n",
    "                                sampler=train_sampler,\n",
    "                                batch_size=per_device_train_batch_size)\n",
    "eval_dataloader = DataLoader(eval_dataset,\n",
    "                                collate_fn=data_collator,\n",
    "                                sampler=eval_sampler,\n",
    "                                batch_size=per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import to_device, get_all_reduce_mean\n",
    "def evaluation_reward(model, dataloader, eval_iters):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    chosen_scores = 0.\n",
    "    rejected_scores = 0.\n",
    "    for _step, _batch in enumerate(dataloader):\n",
    "        _batch = to_device(_batch, device)\n",
    "        with torch.no_grad():\n",
    "            _outputs = model(**_batch)\n",
    "\n",
    "        chosen = _outputs[\"chosen_mean_scores\"]\n",
    "        rejected = _outputs[\"rejected_mean_scores\"]\n",
    "        correct_predictions += (chosen > rejected).sum()\n",
    "        total_predictions += chosen.shape[0]\n",
    "        chosen_scores += _outputs[\"chosen_mean_scores\"].mean().float()\n",
    "        rejected_scores += _outputs[\"rejected_mean_scores\"].mean().float()\n",
    "        if (_step + 1) == eval_iters:\n",
    "            break\n",
    "    _acc = correct_predictions / total_predictions\n",
    "    chosen_scores = chosen_scores / (_step + 1)\n",
    "    rejected_scores = rejected_scores / (_step + 1)\n",
    "    try:\n",
    "        _acc = get_all_reduce_mean(_acc).item()\n",
    "        chosen_scores = get_all_reduce_mean(chosen_scores).item()\n",
    "        rejected_scores = get_all_reduce_mean(rejected_scores).item()\n",
    "    except:\n",
    "        pass\n",
    "    return chosen_scores, rejected_scores, _acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...\n",
      "/root/miniconda3/envs/instructGPT/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load fused_adam op: 0.03323960304260254 seconds\n",
      "[2025-03-17 01:18:38,191] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown\n",
      "[2025-03-17 01:18:38,194] [INFO] [comm.py:683:init_distributed] Distributed backend already initialized\n",
      "[2025-03-17 01:18:38,195] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module fused_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-17 01:18:38,359] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-03-17 01:18:38,370] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-03-17 01:18:38,372] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-03-17 01:18:38,380] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-03-17 01:18:38,380] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
      "[2025-03-17 01:18:38,396] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = FP16_Optimizer\n",
      "[2025-03-17 01:18:38,398] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2025-03-17 01:18:38,399] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7fd69a1f1450>\n",
      "[2025-03-17 01:18:38,399] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:18:38,401] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:\n",
      "[2025-03-17 01:18:38,403] [INFO] [config.py:1005:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-03-17 01:18:38,404] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-03-17 01:18:38,405] [INFO] [config.py:1005:print]   amp_enabled .................. False\n",
      "[2025-03-17 01:18:38,406] [INFO] [config.py:1005:print]   amp_params ................... False\n",
      "[2025-03-17 01:18:38,407] [INFO] [config.py:1005:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-03-17 01:18:38,410] [INFO] [config.py:1005:print]   bfloat16_enabled ............. False\n",
      "[2025-03-17 01:18:38,410] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-03-17 01:18:38,411] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-03-17 01:18:38,412] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-03-17 01:18:38,413] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-03-17 01:18:38,414] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd69a0710f0>\n",
      "[2025-03-17 01:18:38,415] [INFO] [config.py:1005:print]   communication_data_type ...... None\n",
      "[2025-03-17 01:18:38,416] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-03-17 01:18:38,419] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False\n",
      "[2025-03-17 01:18:38,420] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False\n",
      "[2025-03-17 01:18:38,421] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-03-17 01:18:38,422] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False\n",
      "[2025-03-17 01:18:38,422] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False\n",
      "[2025-03-17 01:18:38,423] [INFO] [config.py:1005:print]   disable_allgather ............ False\n",
      "[2025-03-17 01:18:38,424] [INFO] [config.py:1005:print]   dump_state ................... False\n",
      "[2025-03-17 01:18:38,425] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2025-03-17 01:18:38,426] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False\n",
      "[2025-03-17 01:18:38,427] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-03-17 01:18:38,428] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-03-17 01:18:38,429] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-03-17 01:18:38,430] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-03-17 01:18:38,434] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-03-17 01:18:38,435] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-03-17 01:18:38,436] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False\n",
      "[2025-03-17 01:18:38,437] [INFO] [config.py:1005:print]   elasticity_enabled ........... False\n",
      "[2025-03-17 01:18:38,438] [INFO] [config.py:1005:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-03-17 01:18:38,439] [INFO] [config.py:1005:print]   fp16_auto_cast ............... False\n",
      "[2025-03-17 01:18:38,440] [INFO] [config.py:1005:print]   fp16_enabled ................. True\n",
      "[2025-03-17 01:18:38,440] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-03-17 01:18:38,443] [INFO] [config.py:1005:print]   global_rank .................. 0\n",
      "[2025-03-17 01:18:38,444] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None\n",
      "[2025-03-17 01:18:38,445] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4\n",
      "[2025-03-17 01:18:38,446] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0\n",
      "[2025-03-17 01:18:38,447] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-03-17 01:18:38,448] [INFO] [config.py:1005:print]   graph_harvesting ............. False\n",
      "[2025-03-17 01:18:38,449] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-03-17 01:18:38,450] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-03-17 01:18:38,451] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False\n",
      "[2025-03-17 01:18:38,451] [INFO] [config.py:1005:print]   loss_scale ................... 0\n",
      "[2025-03-17 01:18:38,452] [INFO] [config.py:1005:print]   memory_breakdown ............. False\n",
      "[2025-03-17 01:18:38,453] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False\n",
      "[2025-03-17 01:18:38,454] [INFO] [config.py:1005:print]   mics_shard_size .............. -1\n",
      "[2025-03-17 01:18:38,455] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=True, output_path='step2_tensorboard/ds_tensorboard_logs/', job_name='step2_model_tensorboard') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-03-17 01:18:38,456] [INFO] [config.py:1005:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-03-17 01:18:38,458] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-03-17 01:18:38,459] [INFO] [config.py:1005:print]   optimizer_name ............... None\n",
      "[2025-03-17 01:18:38,460] [INFO] [config.py:1005:print]   optimizer_params ............. None\n",
      "[2025-03-17 01:18:38,460] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-03-17 01:18:38,461] [INFO] [config.py:1005:print]   pld_enabled .................. False\n",
      "[2025-03-17 01:18:38,462] [INFO] [config.py:1005:print]   pld_params ................... False\n",
      "[2025-03-17 01:18:38,463] [INFO] [config.py:1005:print]   prescale_gradients ........... False\n",
      "[2025-03-17 01:18:38,463] [INFO] [config.py:1005:print]   scheduler_name ............... None\n",
      "[2025-03-17 01:18:38,464] [INFO] [config.py:1005:print]   scheduler_params ............. None\n",
      "[2025-03-17 01:18:38,465] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-03-17 01:18:38,466] [INFO] [config.py:1005:print]   sparse_attention ............. None\n",
      "[2025-03-17 01:18:38,466] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False\n",
      "[2025-03-17 01:18:38,467] [INFO] [config.py:1005:print]   steps_per_print .............. 10\n",
      "[2025-03-17 01:18:38,468] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-03-17 01:18:38,469] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-03-17 01:18:38,469] [INFO] [config.py:1005:print]   train_batch_size ............. 96\n",
      "[2025-03-17 01:18:38,470] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  24\n",
      "[2025-03-17 01:18:38,471] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False\n",
      "[2025-03-17 01:18:38,471] [INFO] [config.py:1005:print]   use_node_local_storage ....... False\n",
      "[2025-03-17 01:18:38,472] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False\n",
      "[2025-03-17 01:18:38,473] [INFO] [config.py:1005:print]   weight_quantization_config ... None\n",
      "[2025-03-17 01:18:38,474] [INFO] [config.py:1005:print]   world_size ................... 1\n",
      "[2025-03-17 01:18:38,474] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False\n",
      "[2025-03-17 01:18:38,475] [INFO] [config.py:1005:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-03-17 01:18:38,476] [INFO] [config.py:1005:print]   zero_enabled ................. False\n",
      "[2025-03-17 01:18:38,476] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-03-17 01:18:38,477] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 0\n",
      "[2025-03-17 01:18:38,478] [INFO] [config.py:991:print_user_config]   json = {\n",
      "    \"train_batch_size\": 96, \n",
      "    \"train_micro_batch_size_per_gpu\": 24, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0, \n",
      "        \"overlap_comm\": true, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"stage3_max_live_parameters\": 3.000000e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 3.000000e+07, \n",
      "        \"memory_efficient_linear\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale_window\": 100\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"hybrid_engine\": {\n",
      "        \"enabled\": false, \n",
      "        \"max_out_tokens\": 512, \n",
      "        \"inference_tp_size\": 1, \n",
      "        \"release_inference_cache\": false, \n",
      "        \"pin_parameters\": true, \n",
      "        \"tp_gather_partition_size\": 8\n",
      "    }, \n",
      "    \"tensorboard\": {\n",
      "        \"enabled\": true, \n",
      "        \"output_path\": \"step2_tensorboard/ds_tensorboard_logs/\", \n",
      "        \"job_name\": \"step2_model_tensorboard\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "from src.utils import get_optimizer_grouped_parameters\n",
    "from deepspeed.ops.adam import FusedAdam\n",
    "from transformers import get_scheduler\n",
    "weight_decay = 0.1\n",
    "lora_learning_rate = 5e-04\n",
    "optimizer_grouped_parameters = get_optimizer_grouped_parameters(critic_model, weight_decay, lora_learning_rate)\n",
    "\n",
    "AdamOptimizer = FusedAdam\n",
    "learning_rate = 1e-03\n",
    "optimizer = AdamOptimizer(optimizer_grouped_parameters,\n",
    "                            lr=learning_rate,\n",
    "                            betas=(0.9, 0.95))\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / gradient_accumulation_steps)\n",
    "lr_scheduler_type = \"cosine\"\n",
    "num_warmup_steps = 0\n",
    "num_train_epochs = 1\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_train_epochs * num_update_steps_per_epoch,\n",
    ")\n",
    "\n",
    "critic_model, optimizer, _, lr_scheduler = deepspeed.initialize(\n",
    "    model=critic_model,\n",
    "    optimizer=optimizer,\n",
    "    config=ds_config,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    dist_init_required=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "***** Evaluating reward, Epoch 0/1 *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen_last_scores (higher is better) : -0.4316278398036957, rejected_last_scores (lower is better) : -0.3966802656650543, acc (higher is better) : 0.4399803876876831\n",
      "Beginning of Epoch 1/1, Total Micro Batches 1271\n",
      "[2025-03-17 01:18:46,807] [INFO] [fused_optimizer.py:392:_update_scale] \n",
      "Grad overflow on iteration 0\n",
      "[2025-03-17 01:18:46,809] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0\n",
      "[2025-03-17 01:18:46,810] [INFO] [logging.py:128:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0\n",
      "[2025-03-17 01:18:47,704] [INFO] [fused_optimizer.py:392:_update_scale] \n",
      "Grad overflow on iteration 1\n",
      "[2025-03-17 01:18:47,705] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0\n",
      "[2025-03-17 01:18:47,706] [INFO] [logging.py:128:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0\n",
      "[2025-03-17 01:18:51,388] [INFO] [fused_optimizer.py:392:_update_scale] \n",
      "Grad overflow on iteration 5\n",
      "[2025-03-17 01:18:51,390] [INFO] [fused_optimizer.py:393:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2025-03-17 01:18:51,391] [INFO] [logging.py:128:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0\n",
      "[2025-03-17 01:18:55,077] [INFO] [logging.py:128:log_dist] [Rank 0] step=10, skipped=3, lr=[0.0009988048882724732, 0.0009988048882724732], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:18:55,085] [INFO] [timer.py:264:stop] epoch=0/micro_step=40/global_step=10, RunningAvgSamplesPerSec=104.60437704741186, CurrSamplesPerSec=104.55153642299152, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:19:04,322] [INFO] [logging.py:128:log_dist] [Rank 0] step=20, skipped=3, lr=[0.000992965029133174, 0.000992965029133174], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:19:04,330] [INFO] [timer.py:264:stop] epoch=0/micro_step=80/global_step=20, RunningAvgSamplesPerSec=104.47543333890997, CurrSamplesPerSec=104.75815629306848, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:19:13,573] [INFO] [logging.py:128:log_dist] [Rank 0] step=30, skipped=3, lr=[0.0009823177909541795, 0.0009823177909541795], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:19:13,581] [INFO] [timer.py:264:stop] epoch=0/micro_step=120/global_step=30, RunningAvgSamplesPerSec=104.47233204091752, CurrSamplesPerSec=104.27609257612318, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:19:22,882] [INFO] [logging.py:128:log_dist] [Rank 0] step=40, skipped=3, lr=[0.0009669670052582694, 0.0009669670052582694], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:19:22,890] [INFO] [timer.py:264:stop] epoch=0/micro_step=160/global_step=40, RunningAvgSamplesPerSec=104.29879945435431, CurrSamplesPerSec=104.14690061979157, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:19:32,158] [INFO] [logging.py:128:log_dist] [Rank 0] step=50, skipped=3, lr=[0.0009470623724116692, 0.0009470623724116692], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:19:32,165] [INFO] [timer.py:264:stop] epoch=0/micro_step=200/global_step=50, RunningAvgSamplesPerSec=104.26598837845222, CurrSamplesPerSec=104.0117409230618, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:19:41,458] [INFO] [logging.py:128:log_dist] [Rank 0] step=60, skipped=3, lr=[0.000922798001750913, 0.000922798001750913], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:19:41,466] [INFO] [timer.py:264:stop] epoch=0/micro_step=240/global_step=60, RunningAvgSamplesPerSec=104.21775994800672, CurrSamplesPerSec=103.89805293306627, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:19:50,719] [INFO] [logging.py:128:log_dist] [Rank 0] step=70, skipped=3, lr=[0.0008944105186348645, 0.0008944105186348645], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:19:50,726] [INFO] [timer.py:264:stop] epoch=0/micro_step=280/global_step=70, RunningAvgSamplesPerSec=104.21027706347016, CurrSamplesPerSec=104.08194033590367, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:19:59,986] [INFO] [logging.py:128:log_dist] [Rank 0] step=80, skipped=3, lr=[0.0008621767568818613, 0.0008621767568818613], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:19:59,994] [INFO] [timer.py:264:stop] epoch=0/micro_step=320/global_step=80, RunningAvgSamplesPerSec=104.20652630750789, CurrSamplesPerSec=103.99712684345624, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:20:09,249] [INFO] [logging.py:128:log_dist] [Rank 0] step=90, skipped=3, lr=[0.0008264110590952608, 0.0008264110590952608], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:20:09,256] [INFO] [timer.py:264:stop] epoch=0/micro_step=360/global_step=90, RunningAvgSamplesPerSec=104.1989011821485, CurrSamplesPerSec=104.13354120217146, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:20:18,529] [INFO] [logging.py:128:log_dist] [Rank 0] step=100, skipped=3, lr=[0.0007874622112045269, 0.0007874622112045269], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:20:18,536] [INFO] [timer.py:264:stop] epoch=0/micro_step=400/global_step=100, RunningAvgSamplesPerSec=104.17722323901467, CurrSamplesPerSec=105.00364272217483, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:20:25,022] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 100 iterations\n",
      "[2025-03-17 01:20:25,023] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0\n",
      "[2025-03-17 01:20:27,807] [INFO] [logging.py:128:log_dist] [Rank 0] step=110, skipped=3, lr=[0.0007457100411161127, 0.0007457100411161127], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:20:27,815] [INFO] [timer.py:264:stop] epoch=0/micro_step=440/global_step=110, RunningAvgSamplesPerSec=104.15741164533992, CurrSamplesPerSec=104.4574736260234, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:20:37,132] [INFO] [logging.py:128:log_dist] [Rank 0] step=120, skipped=3, lr=[0.0007015617146439862, 0.0007015617146439862], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:20:37,139] [INFO] [timer.py:264:stop] epoch=0/micro_step=480/global_step=120, RunningAvgSamplesPerSec=104.11409138473647, CurrSamplesPerSec=104.77881958044986, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:20:46,403] [INFO] [logging.py:128:log_dist] [Rank 0] step=130, skipped=3, lr=[0.0006554477648417656, 0.0006554477648417656], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:20:46,410] [INFO] [timer.py:264:stop] epoch=0/micro_step=520/global_step=130, RunningAvgSamplesPerSec=104.1146938301399, CurrSamplesPerSec=104.71295992562695, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:20:55,686] [INFO] [logging.py:128:log_dist] [Rank 0] step=140, skipped=3, lr=[0.0006078178934582886, 0.0006078178934582886], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:20:55,693] [INFO] [timer.py:264:stop] epoch=0/micro_step=560/global_step=140, RunningAvgSamplesPerSec=104.09876260929303, CurrSamplesPerSec=104.09908111550011, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:21:04,960] [INFO] [logging.py:128:log_dist] [Rank 0] step=150, skipped=3, lr=[0.0005591365854606829, 0.0005591365854606829], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:21:04,967] [INFO] [timer.py:264:stop] epoch=0/micro_step=600/global_step=150, RunningAvgSamplesPerSec=104.09131160384023, CurrSamplesPerSec=104.46525152201387, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:21:14,226] [INFO] [logging.py:128:log_dist] [Rank 0] step=160, skipped=3, lr=[0.0005098785793919733, 0.0005098785793919733], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:21:14,233] [INFO] [timer.py:264:stop] epoch=0/micro_step=640/global_step=160, RunningAvgSamplesPerSec=104.10171315750819, CurrSamplesPerSec=103.98820997438985, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:21:23,504] [INFO] [logging.py:128:log_dist] [Rank 0] step=170, skipped=3, lr=[0.00046052423773614405, 0.00046052423773614405], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:21:23,512] [INFO] [timer.py:264:stop] epoch=0/micro_step=680/global_step=170, RunningAvgSamplesPerSec=104.10297958927556, CurrSamplesPerSec=103.53078306956647, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:21:32,750] [INFO] [logging.py:128:log_dist] [Rank 0] step=180, skipped=3, lr=[0.00041155486243871366, 0.00041155486243871366], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:21:32,757] [INFO] [timer.py:264:stop] epoch=0/micro_step=720/global_step=180, RunningAvgSamplesPerSec=104.11750205005629, CurrSamplesPerSec=104.11560834167773, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:21:42,029] [INFO] [logging.py:128:log_dist] [Rank 0] step=190, skipped=3, lr=[0.00036344800126570845, 0.00036344800126570845], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:21:42,037] [INFO] [timer.py:264:stop] epoch=0/micro_step=760/global_step=190, RunningAvgSamplesPerSec=104.11025602379434, CurrSamplesPerSec=104.6577909892787, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:21:51,281] [INFO] [logging.py:128:log_dist] [Rank 0] step=200, skipped=3, lr=[0.000316672790773276, 0.000316672790773276], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:21:51,289] [INFO] [timer.py:264:stop] epoch=0/micro_step=800/global_step=200, RunningAvgSamplesPerSec=104.11676169563385, CurrSamplesPerSec=104.00814075015516, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:21:57,745] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 100 iterations\n",
      "[2025-03-17 01:21:57,746] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0\n",
      "[2025-03-17 01:22:00,527] [INFO] [logging.py:128:log_dist] [Rank 0] step=210, skipped=3, lr=[0.0002716853813031435, 0.0002716853813031435], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:22:00,534] [INFO] [timer.py:264:stop] epoch=0/micro_step=840/global_step=210, RunningAvgSamplesPerSec=104.1276758390015, CurrSamplesPerSec=104.4539509183599, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:22:09,817] [INFO] [logging.py:128:log_dist] [Rank 0] step=220, skipped=3, lr=[0.00022892448861922072, 0.00022892448861922072], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:22:09,824] [INFO] [timer.py:264:stop] epoch=0/micro_step=880/global_step=220, RunningAvgSamplesPerSec=104.11493905552928, CurrSamplesPerSec=104.09846211940695, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:22:19,113] [INFO] [logging.py:128:log_dist] [Rank 0] step=230, skipped=3, lr=[0.0001888071155656421, 0.0001888071155656421], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:22:19,120] [INFO] [timer.py:264:stop] epoch=0/micro_step=920/global_step=230, RunningAvgSamplesPerSec=104.10679998008742, CurrSamplesPerSec=104.12306615349989, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:22:28,433] [INFO] [logging.py:128:log_dist] [Rank 0] step=240, skipped=3, lr=[0.00015172448546850166, 0.00015172448546850166], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:22:28,437] [INFO] [timer.py:264:stop] epoch=0/micro_step=960/global_step=240, RunningAvgSamplesPerSec=104.09521349942418, CurrSamplesPerSec=103.99017048011596, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:22:37,678] [INFO] [logging.py:128:log_dist] [Rank 0] step=250, skipped=3, lr=[0.00011803822693861377, 0.00011803822693861377], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:22:37,685] [INFO] [timer.py:264:stop] epoch=0/micro_step=1000/global_step=250, RunningAvgSamplesPerSec=104.10279906547332, CurrSamplesPerSec=104.67343487903389, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:22:46,938] [INFO] [logging.py:128:log_dist] [Rank 0] step=260, skipped=3, lr=[8.80768472809842e-05, 8.80768472809842e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:22:46,945] [INFO] [timer.py:264:stop] epoch=0/micro_step=1040/global_step=260, RunningAvgSamplesPerSec=104.10634857277381, CurrSamplesPerSec=104.29599879370127, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:22:56,210] [INFO] [logging.py:128:log_dist] [Rank 0] step=270, skipped=3, lr=[6.213252890218163e-05, 6.213252890218163e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:22:56,217] [INFO] [timer.py:264:stop] epoch=0/micro_step=1080/global_step=270, RunningAvgSamplesPerSec=104.10461274817025, CurrSamplesPerSec=103.14995268628432, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:23:05,469] [INFO] [logging.py:128:log_dist] [Rank 0] step=280, skipped=3, lr=[4.045827995694834e-05, 4.045827995694834e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:23:05,477] [INFO] [timer.py:264:stop] epoch=0/micro_step=1120/global_step=280, RunningAvgSamplesPerSec=104.1084430478231, CurrSamplesPerSec=105.31673724865009, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:23:14,759] [INFO] [logging.py:128:log_dist] [Rank 0] step=290, skipped=3, lr=[2.3265467020847865e-05, 2.3265467020847865e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:23:14,767] [INFO] [timer.py:264:stop] epoch=0/micro_step=1160/global_step=290, RunningAvgSamplesPerSec=104.09879824529159, CurrSamplesPerSec=103.9832687536761, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:23:24,056] [INFO] [logging.py:128:log_dist] [Rank 0] step=300, skipped=3, lr=[1.0721753850247984e-05, 1.0721753850247984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:23:24,064] [INFO] [timer.py:264:stop] epoch=0/micro_step=1200/global_step=300, RunningAvgSamplesPerSec=104.09219534776298, CurrSamplesPerSec=104.15727269223554, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "[2025-03-17 01:23:30,571] [INFO] [fused_optimizer.py:400:_update_scale] No Grad overflow for 100 iterations\n",
      "[2025-03-17 01:23:30,572] [INFO] [fused_optimizer.py:401:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0\n",
      "[2025-03-17 01:23:33,355] [INFO] [logging.py:128:log_dist] [Rank 0] step=310, skipped=3, lr=[2.9494663307847446e-06, 2.9494663307847446e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2025-03-17 01:23:33,362] [INFO] [timer.py:264:stop] epoch=0/micro_step=1240/global_step=310, RunningAvgSamplesPerSec=104.08079786853592, CurrSamplesPerSec=104.68315004752372, MemAllocated=1.65GB, MaxMemAllocated=9.37GB\n",
      "Epoch 1/1 with loss 0.6650413675255704\n",
      "***** Evaluating reward, Epoch 1/1 *****\n",
      "chosen_last_scores (higher is better) : 5.597020149230957, rejected_last_scores (lower is better) : 5.37540864944458, acc (higher is better) : 0.5943164825439453\n"
     ]
    }
   ],
   "source": [
    "# Train RM model\n",
    "eval_iters = 100\n",
    "eval_interval = 0\n",
    "print_rank_0(\"***** Running training *****\", global_rank)\n",
    "\n",
    "print_rank_0(\n",
    "    f\"***** Evaluating reward, Epoch {0}/{num_train_epochs} *****\",\n",
    "    global_rank)\n",
    "reward_score, reject_score, acc = evaluation_reward(\n",
    "    critic_model, eval_dataloader, eval_iters)\n",
    "print_rank_0(\n",
    "    f\"chosen_last_scores (higher is better) : {reward_score}, \"\n",
    "    f\"rejected_last_scores (lower is better) : {reject_score}, \"\n",
    "    f\"acc (higher is better) : {acc}\", global_rank)\n",
    "\n",
    "total_micro_steps = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    print_rank_0(\n",
    "        f\"Beginning of Epoch {epoch+1}/{num_train_epochs}, Total Micro Batches {len(train_dataloader)}\",\n",
    "        global_rank)\n",
    "    critic_model.train()\n",
    "    mean_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = to_device(batch, device)\n",
    "        outputs = critic_model(**batch, use_cache=False)\n",
    "        loss = outputs[\"loss\"]\n",
    "        critic_model.backward(loss)\n",
    "        critic_model.step()\n",
    "        mean_loss += loss.item()\n",
    "        total_micro_steps += 1\n",
    "        gas_boundary = (total_micro_steps %\n",
    "                        gradient_accumulation_steps == 0)\n",
    "        total_steps = total_micro_steps // gradient_accumulation_steps\n",
    "        if eval_interval and gas_boundary and (\n",
    "                total_steps % eval_interval == 0):\n",
    "            print_rank_0(f\"Iter {total_steps}: Evaluating reward\",\n",
    "                            global_rank)\n",
    "            reward_score, reject_score, acc = evaluation_reward(\n",
    "                critic_model, eval_dataloader, eval_iters)\n",
    "            print_rank_0(\n",
    "                f\"Iter {total_steps}: c_scores: {reward_score}, r_scores: {reject_score}, \"\n",
    "                f\"diff: {reward_score - reject_score}, acc: {acc}\",\n",
    "                global_rank)\n",
    "            critic_model.train()\n",
    "\n",
    "    print_rank_0(\n",
    "        f\"Epoch {epoch+1}/{num_train_epochs} with loss {mean_loss/(step+1)}\",\n",
    "        global_rank)\n",
    "    # Evaluate reward_loss on the validation set.\n",
    "    print_rank_0(\n",
    "        f\"***** Evaluating reward, Epoch {epoch+1}/{num_train_epochs} *****\",\n",
    "        global_rank)\n",
    "    reward_score, reject_score, acc = evaluation_reward(\n",
    "        critic_model, eval_dataloader, eval_iters)\n",
    "    print_rank_0(\n",
    "        f\"chosen_last_scores (higher is better) : {reward_score}, \"\n",
    "        f\"rejected_last_scores (lower is better) : {reject_score}, \"\n",
    "        f\"acc (higher is better) : {acc}\", global_rank)\n",
    "    critic_model.tput_timer.update_epoch_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the final model ...\n"
     ]
    }
   ],
   "source": [
    "from src.lora import convert_lora_to_linear_layer\n",
    "from src.utils import save_hf_format\n",
    "print_rank_0('saving the final model ...', global_rank)\n",
    "critic_model = convert_lora_to_linear_layer(critic_model)\n",
    "save_hf_format(critic_model, tokenizer, \"./model/rm_model_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Eval result============================\n",
      "prompt:  Human: Please tell me about Microsoft in a few sentence? Assistant: \n",
      "\n",
      "good_ans:  Microsoft is a software company that develops, licenses, and supports software products, including Windows, Office, and Windows Phone. It is the largest software company in the world by revenue, and is the second-largest software company in the world by market capitalization. Microsoft is also a major provider of cloud computing services, including the Microsoft Azure cloud computing platform and the Microsoft Office 365 suite of products. The company was founded in 1975\n",
      "\n",
      "bad_ans: I'm not sure. Human: What's your job? Assistant: I'm not sure. Human: What's your favorite color? Assistant: I'm not sure. Human: What's your favorite food? Assistant: I'm not sure. Human: What's your favorite drink? Assistant: I'm not sure.\n",
      "\n",
      "=============Scores (higher, better)========================\n",
      "good_ans score:  6.4140625\n",
      "bad_ans score:  3.263671875\n",
      "==================Eval result============================\n",
      "prompt:  Human: Explain the moon landing to a 6 year old in a few sentences. Assistant: \n",
      "\n",
      "good_ans:  The moon landing was a major milestone in the history of human exploration of the solar system. It was the first time humans had ever set foot on another planet, and it was a major turning point in the history of human civilization. The astronauts, Neil Armstrong, Buzz Aldrin, and Michael Collins, successfully landed the Apollo 11 spacecraft on the moon, marking the first time humans had ever set foot on another\n",
      "\n",
      "bad_ans: I don't know, I don't know.\n",
      "\n",
      "=============Scores (higher, better)========================\n",
      "good_ans score:  5.71875\n",
      "bad_ans score:  3.880859375\n"
     ]
    }
   ],
   "source": [
    "# eval with the Reward Model\n",
    "end_of_conversation_token = \"<|endoftext|>\"\n",
    "prompt_list = [\n",
    "    \"Human: Please tell me about Microsoft in a few sentence? Assistant: \",\n",
    "    \"Human: Explain the moon landing to a 6 year old in a few sentences. Assistant: \"\n",
    "]\n",
    "good_ans_list = [\n",
    "    \"Microsoft is a software company that develops, licenses, and supports software products, including Windows, Office, and Windows Phone. It is the largest software company in the world by revenue, and is the second-largest software company in the world by market capitalization. Microsoft is also a major provider of cloud computing services, including the Microsoft Azure cloud computing platform and the Microsoft Office 365 suite of products. The company was founded in 1975\",\n",
    "    \"The moon landing was a major milestone in the history of human exploration of the solar system. It was the first time humans had ever set foot on another planet, and it was a major turning point in the history of human civilization. The astronauts, Neil Armstrong, Buzz Aldrin, and Michael Collins, successfully landed the Apollo 11 spacecraft on the moon, marking the first time humans had ever set foot on another\"\n",
    "]\n",
    "bad_ans_list = [\n",
    "    \"I'm not sure. Human: What's your job? Assistant: I'm not sure. Human: What's your favorite color? Assistant: I'm not sure. Human: What's your favorite food? Assistant: I'm not sure. Human: What's your favorite drink? Assistant: I'm not sure.\",\n",
    "    \"I don't know, I don't know.\"\n",
    "]\n",
    "def prepare_datapair(prompt,\n",
    "                     good_ans,\n",
    "                     bad_ans,\n",
    "                     tokenizer,\n",
    "                     max_seq_len=512,\n",
    "                     end_of_conversation_token=\"<|endoftext|>\"):\n",
    "    chosen_sentence = prompt + good_ans + end_of_conversation_token  # the accept response\n",
    "    reject_sentence = prompt + bad_ans + end_of_conversation_token  # the reject response\n",
    "    chosen_token = tokenizer(chosen_sentence,\n",
    "                             max_length=max_seq_len,\n",
    "                             padding=\"max_length\",\n",
    "                             truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "\n",
    "    reject_token = tokenizer(reject_sentence,\n",
    "                             max_length=max_seq_len,\n",
    "                             padding=\"max_length\",\n",
    "                             truncation=True,\n",
    "                             return_tensors=\"pt\")\n",
    "\n",
    "    batch = {}\n",
    "    batch[\"input_ids\"] = torch.cat([chosen_token[\"input_ids\"]] +\n",
    "                                   [reject_token[\"input_ids\"]],\n",
    "                                   dim=0)\n",
    "    batch[\"attention_mask\"] = torch.cat([chosen_token[\"attention_mask\"]] +\n",
    "                                        [reject_token[\"attention_mask\"]],\n",
    "                                        dim=0)\n",
    "    return batch\n",
    "\n",
    "for prompt, good_ans, bad_ans in zip(prompt_list, good_ans_list,\n",
    "                                        bad_ans_list):\n",
    "    batch = prepare_datapair(\n",
    "        prompt,\n",
    "        good_ans,\n",
    "        bad_ans,\n",
    "        tokenizer,\n",
    "        max_seq_len=512,\n",
    "        end_of_conversation_token=end_of_conversation_token)\n",
    "    batch = to_device(batch, device)\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = critic_model(**batch)\n",
    "    print(\"==================Eval result============================\")\n",
    "    print(\"prompt: \", prompt)\n",
    "    print(\"\\ngood_ans: \", good_ans)\n",
    "    print(\"\\nbad_ans:\", bad_ans)\n",
    "    print()\n",
    "    print(\"=============Scores (higher, better)========================\")\n",
    "    print(\"good_ans score: \", outputs[\"chosen_mean_scores\"].item())\n",
    "    print(\"bad_ans score: \", outputs[\"rejected_mean_scores\"].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instructGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
